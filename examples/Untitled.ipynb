{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50af6749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sandpyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4878add",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from itertools import combinations as comb\n",
    "from geopandas.tools import overlay\n",
    "import os\n",
    "from sandpyper.common import coords_to_points\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rasterio as ras\n",
    "from rasterio.plot import show\n",
    "\n",
    "from sandpyper.sandpyper import ProfileSet\n",
    "from sandpyper.common import get_sil_location, get_opt_k\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "crs_dict_string= {\n",
    "                 'mar': {'init': 'epsg:32754'},\n",
    "                 'leo':{'init': 'epsg:32755'}\n",
    "                 }\n",
    "\n",
    "\n",
    "def kmeans_sa(merged_df, ks, feature_set, thresh_k=5, random_state=10):\n",
    "    \"\"\"\n",
    "    Function to use KMeans on all surveys with the optimal k obtained from the Silhouette Analysis.\n",
    "    It uses KMeans as a clusterer.\n",
    "\n",
    "    Args:\n",
    "        merged_df (Pandas dataframe): The clean and merged dataframe containing the features. Must contain the columns point_id, location and survey_date, as well as the\n",
    "        ks (int, dict): number of clusters (k) or dictionary containing the optimal k for each survey. See get_opt_k function.\n",
    "        feature_set (list): List of names of features in the dataframe to use for clustering.\n",
    "        thresh_k (int): Minimim k to be used. If survey-specific optimal k is below this value, then k equals the average k of all above threshold values.\n",
    "        random_state (int): Random seed used to make the randomisation deterministic.\n",
    "\n",
    "    Returns:\n",
    "        data_classified (pd.DataFrame): A dataframe containing the label_k column, with point_id, location, survey_date and the features used to cluster the data.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    merged_df.dropna(inplace=True)\n",
    "    list_locs = merged_df.location.unique()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    data_classified = pd.DataFrame()\n",
    "\n",
    "    # Set a threshold k, in case a k is lower than 5, use the mean optimal k\n",
    "    # of the other surveys above threshold\n",
    "\n",
    "    # # Compute the mean optimal k of above threshold ks\n",
    "    if isinstance(ks, dict):\n",
    "        arr_k = np.array([i for i in ks.values() if i > thresh_k])\n",
    "        mean_threshold_k = np.int(np.round(np.mean(arr_k), 0))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    for location in tqdm(list_locs):\n",
    "\n",
    "        list_dates = merged_df.query(f\"location=='{location}'\").raw_date.unique()\n",
    "\n",
    "        for survey_date_in in tqdm(list_dates):\n",
    "\n",
    "            data_in = merged_df.query(\n",
    "                f\"location=='{location}'& raw_date == {survey_date_in}\"\n",
    "            )\n",
    "            data_clean = data_in[feature_set].apply(pd.to_numeric)\n",
    "\n",
    "            if isinstance(ks, dict):\n",
    "                k = ks[f\"{location}_{survey_date_in}\"]\n",
    "            else:\n",
    "                k=ks\n",
    "\n",
    "            if k < thresh_k:\n",
    "                k = mean_threshold_k\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            minmax_scaled_df = scaler.fit_transform(np.nan_to_num(data_clean))\n",
    "\n",
    "            clusterer = KMeans(\n",
    "                n_clusters=k,\n",
    "                init=\"k-means++\",\n",
    "                algorithm=\"elkan\",\n",
    "                tol=0.0001,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "\n",
    "            data_in[\"label_k\"] = clusterer.fit_predict(minmax_scaled_df)\n",
    "\n",
    "            data_classified = pd.concat(\n",
    "                [data_in, data_classified], ignore_index=True\n",
    "            )\n",
    "\n",
    "    return data_classified\n",
    "\n",
    "\n",
    "def check_dicts_duplicated_values(l_dicts):\n",
    "\n",
    "    dict_check = {}\n",
    "    dict_dups = {}\n",
    "    all_dicts=[dicto for dicto in l_dicts.values()]\n",
    "\n",
    "    for dict_in in all_dicts:\n",
    "        for key in set().union(*all_dicts):\n",
    "            if key in dict_in:\n",
    "                dict_check.setdefault(key, []).extend(dict_in[key])\n",
    "\n",
    "    for survey, labels in dict_check.items():\n",
    "        duplicated=[x for x in labels if labels.count(x) > 1]\n",
    "        if len(duplicated)>=1:\n",
    "            dict_dups.update({survey:set(set(duplicated))})\n",
    "\n",
    "    if len(dict_dups)>0:\n",
    "        raise ValueError(f\"Duplicated label_k found in the following dictionaries.\\n\\n{dict_dups}\\n\\nPlease revise and assigned those labels_k to only one class dictionary.\")\n",
    "\n",
    "\n",
    "\n",
    "def classify_labelk(labelled_dataset,l_dicts, cluster_field='label_k', fill_class='sand'):\n",
    "\n",
    "    check_dicts_duplicated_values(l_dicts)\n",
    "\n",
    "    labelled_dataset[\"pt_class\"]=np.nan\n",
    "\n",
    "    all_keys = set().union(*(d.keys() for d in [i for i in l_dicts.values()]))\n",
    "    class_names=l_dicts.keys()\n",
    "\n",
    "    classed_df=pd.DataFrame()\n",
    "\n",
    "    for loc in labelled_dataset.location.unique():\n",
    "        data_in_loc=labelled_dataset.query(f\"location=='{loc}'\")[[\"location\",\"raw_date\",cluster_field,\"pt_class\",'point_id']]\n",
    "\n",
    "        for raw_date in data_in_loc.raw_date.unique():\n",
    "            loc_date_tag=f\"{loc}_{raw_date}\"\n",
    "            data_in=data_in_loc.query(f\"raw_date=={raw_date}\")\n",
    "\n",
    "            if loc_date_tag in all_keys:\n",
    "\n",
    "                for class_in in class_names:\n",
    "\n",
    "                    if loc_date_tag in l_dicts[class_in].keys():\n",
    "                        loc_date_class_values=l_dicts[class_in][loc_date_tag]\n",
    "\n",
    "                        if len(loc_date_class_values)>=1:\n",
    "                            tmp_dict={label_k:class_in for label_k in loc_date_class_values}\n",
    "                            data_in['pt_class'].update(data_in[cluster_field].map(tmp_dict))\n",
    "\n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                print(f\"{loc_date_tag} not in the class dictionaries. All their labels assigned to fill_class {fill_class}.\")\n",
    "                data_in[\"pt_class\"].fillna(fill_class, inplace=True)\n",
    "\n",
    "            classed_df=pd.concat([classed_df,data_in], ignore_index=True)\n",
    "\n",
    "    merged=labelled_dataset.iloc[:,:-1].merge(right=classed_df[['point_id','pt_class']], on='point_id', how='left')\n",
    "\n",
    "    merged[\"pt_class\"].fillna(fill_class, inplace=True)\n",
    "    return merged\n",
    "\n",
    "def cleanit(to_clean, l_dicts, cluster_field='label_k', fill_class='sand',\n",
    "            watermasks_path=None, water_label='water',\n",
    "            shoremasks_path=None, label_corrections_path=None,\n",
    "            crs_dict_string=None,\n",
    "           geometry_field='coordinates'):\n",
    "\n",
    "    print(\"Reclassifying dataset with the provided dictionaries.\" )\n",
    "    to_clean_classified=classify_labelk(to_clean, l_dicts)\n",
    "\n",
    "    if watermasks_path==None and shoremasks_path==None and label_corrections_path==None:\n",
    "        print(\"No cleaning polygones have been passed. Returning classified dataset.\")\n",
    "        return to_clean_classified\n",
    "\n",
    "    processes=[]\n",
    "\n",
    "    #______ LABELS FINETUNING_______________\n",
    "\n",
    "    if label_corrections_path != None:\n",
    "        if os.path.isfile(label_corrections_path):\n",
    "            label_corrections=gpd.read_file(label_corrections_path)\n",
    "            print(f\"Label corrections provided in CRS: {label_corrections.crs}\")\n",
    "\n",
    "            # validate label correction polygons\n",
    "            #if crs_dict_string != None:\n",
    "            #    check_overlaps_poly_label(label_corrections,to_clean_classified,crs_dict_string)\n",
    "            #else:\n",
    "            #    check_overlaps_poly_label(label_corrections,to_clean_classified,label_corrections.crs.to_epsg())\n",
    "\n",
    "            processes.append(\"polygon finetuning\")\n",
    "            to_update_finetune=pd.DataFrame()\n",
    "\n",
    "\n",
    "            for loc in label_corrections.location.unique():\n",
    "                print(f\"Fine tuning in {loc}.\")\n",
    "\n",
    "                to_clean_subset_loc=to_clean_classified.query(f\" location == '{loc}'\")\n",
    "\n",
    "                for raw_date in tqdm(label_corrections.query(f\"location=='{loc}'\").raw_date.unique()):\n",
    "\n",
    "                    subset_finetune_polys=label_corrections.query(f\"location=='{loc}' and raw_date== {raw_date}\")\n",
    "\n",
    "                    for i,row in subset_finetune_polys.iterrows(): # loops through all the polygones\n",
    "\n",
    "                        target_k=int(row['target_label_k'])\n",
    "                        new_class=row['new_class']\n",
    "\n",
    "                        if target_k != 999:\n",
    "                            data_in=to_clean_subset_loc.query(f\"raw_date == {raw_date} and label_k== {target_k}\")\n",
    "\n",
    "                        elif target_k == 999:\n",
    "                            data_in=to_clean_subset_loc.query(f\"raw_date == {raw_date}\")\n",
    "\n",
    "                        selection=data_in[data_in.coordinates.intersects(row.geometry)]\n",
    "\n",
    "                        if selection.shape[0]==0:\n",
    "                            selection=data_in[data_in.to_crs(crs_dict_string[loc]).coordinates.intersects(row.geometry)]\n",
    "                        else:\n",
    "                            pass\n",
    "                        selection[\"finetuned_label\"]=new_class\n",
    "\n",
    "                        print(f\"Fine-tuning label_k {target_k} to {new_class} in {loc}-{raw_date}, found {selection.shape[0]} pts.\")\n",
    "                        to_update_finetune=pd.concat([selection,to_update_finetune], ignore_index=True)\n",
    "\n",
    "            classed_df_finetuned=to_clean_classified.merge(right=to_update_finetune.loc[:,['point_id','finetuned_label']], # Left Join\n",
    "                                         how='left', validate='one_to_one')\n",
    "\n",
    "            classed_df_finetuned.finetuned_label.fillna(classed_df_finetuned.pt_class, inplace=True) # Fill NaN with previous sand labels\n",
    "        else:\n",
    "            raise NameError(\"Label correction file path is invalid.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if shoremasks_path == None and watermasks_path == None:\n",
    "        print(f\"{processes} completed.\")\n",
    "\n",
    "        if 'watermasked_label' in classed_df_finetuned.columns and 'finetuned_label' not in classed_df_finetuned.columns:\n",
    "            classed_df_finetuned['pt_class']=classed_df_finetuned.watermasked_label\n",
    "            classed_df_finetuned.drop(['watermasked_label'], axis=1, inplace=True)\n",
    "\n",
    "        elif 'finetuned_label' in classed_df_finetuned.columns and 'watermasked_label' not in classed_df_finetuned.columns:\n",
    "            classed_df_finetuned['pt_class']=classed_df_finetuned.finetuned_label\n",
    "            classed_df_finetuned.drop(['finetuned_label'], axis=1, inplace=True)\n",
    "\n",
    "        elif 'finetuned_label' in classed_df_finetuned.columns and 'watermasked_label' in classed_df_finetuned.columns:\n",
    "            classed_df_finetuned['pt_class']=classed_df_finetuned.watermasked_label\n",
    "            classed_df_finetuned.drop(['finetuned_label','watermasked_label'], axis=1, inplace=True)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return classed_df_finetuned\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #______ WATERMASKING_______________\n",
    "\n",
    "    if watermasks_path != None:\n",
    "        if os.path.isfile(watermasks_path):\n",
    "            # apply watermasks\n",
    "            watermask=gpd.read_file(watermasks_path)\n",
    "            print(f\"watermask  provided in CRS: {watermask.crs}\")\n",
    "\n",
    "\n",
    "            print(\"Applying watermasks cleaning.\")\n",
    "            processes.append(\"watermasking\")\n",
    "\n",
    "            if \"polygon finetuning\" in processes:\n",
    "                dataset_to_clean=classed_df_finetuned\n",
    "                starting_labels='finetuned_label'\n",
    "            else:\n",
    "                dataset_to_clean=to_clean_classified\n",
    "                starting_labels='pt_class'\n",
    "\n",
    "\n",
    "            to_update_watermasked=pd.DataFrame()\n",
    "\n",
    "            for loc in watermask.location.unique():\n",
    "                print(f\"Watermasking in {loc}.\")\n",
    "\n",
    "                for raw_date in tqdm(watermask.query(f\"location=='{loc}'\").raw_date.unique()):\n",
    "\n",
    "                    subset_data=dataset_to_clean.query(f\"location=='{loc}' and raw_date == {raw_date}\")\n",
    "                    subset_masks=watermask.query(f\"location=='{loc}' and raw_date == {raw_date}\")\n",
    "\n",
    "                    selection=subset_data[subset_data.geometry.intersects(subset_masks.geometry)]\n",
    "                    if selection.shape[0]==0:\n",
    "                        selection=subset_data[subset_data.geometry.intersects(subset_masks.to_crs(crs_dict_string[loc]).geometry.any())]\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    print(f\"Setting to {water_label} {selection.shape[0]} pts overlapping provided watermasks.\")\n",
    "\n",
    "                    selection[\"watermasked_label\"]=water_label\n",
    "\n",
    "                    to_update_watermasked=pd.concat([selection,to_update_watermasked], ignore_index=True)\n",
    "\n",
    "            classed_df_watermasked=dataset_to_clean.merge(right=to_update_watermasked.loc[:,['point_id','watermasked_label']], # Left Join\n",
    "                                         how='left', validate='one_to_one')\n",
    "            classed_df_watermasked.watermasked_label.fillna(classed_df_watermasked.loc[:,starting_labels], inplace=True) # Fill NaN with previous sand labels\n",
    "\n",
    "            if shoremasks_path == None:\n",
    "                print(f\"{processes} completed.\")\n",
    "\n",
    "                if 'watermasked_label' in classed_df_watermasked.columns and 'finetuned_label' not in classed_df_watermasked.columns:\n",
    "                    classed_df_watermasked['pt_class']=classed_df_watermasked.watermasked_label\n",
    "                    classed_df_watermasked.drop(['watermasked_label'], axis=1, inplace=True)\n",
    "\n",
    "                elif 'finetuned_label' in classed_df_watermasked.columns and 'watermasked_label' not in classed_df_watermasked.columns:\n",
    "                    classed_df_watermasked['pt_class']=classed_df_watermasked.finetuned_label\n",
    "                    classed_df_watermasked.drop(['finetuned_label'], axis=1, inplace=True)\n",
    "\n",
    "                elif 'finetuned_label' in classed_df_watermasked.columns and 'watermasked_label' in classed_df_watermasked.columns:\n",
    "                    classed_df_watermasked['pt_class']=classed_df_watermasked.watermasked_label\n",
    "                    classed_df_watermasked.drop(['finetuned_label','watermasked_label'], axis=1, inplace=True)\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                return classed_df_watermasked\n",
    "        else:\n",
    "            raise NameError(\"watermask file path is invalid.\")\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #______ SHOREMASKING_______________\n",
    "\n",
    "    if shoremasks_path != None:\n",
    "        if os.path.isfile(shoremasks_path):\n",
    "            # apply shoremasks\n",
    "            shoremask=gpd.read_file(shoremasks_path)\n",
    "            print(f\"shoremask  provided in CRS: {shoremask.crs}\")\n",
    "            print(\"Applying shoremasks cleaning.\")\n",
    "            processes.append(\"shoremasking\")\n",
    "\n",
    "\n",
    "            if \"polygon finetuning\" in processes and \"watermasking\" not in processes:\n",
    "                dataset_to_clean=classed_df_finetuned\n",
    "            elif \"polygon finetuning\" not in processes and \"watermasking\" in processes:\n",
    "                dataset_to_clean=classed_df_watermasked\n",
    "            elif \"polygon finetuning\"  in processes and \"watermasking\" in processes:\n",
    "                dataset_to_clean=classed_df_watermasked\n",
    "            else:\n",
    "                dataset_to_clean=to_clean_classified\n",
    "\n",
    "            inshore_cleaned=gpd.GeoDataFrame()\n",
    "            for loc in shoremask.location.unique():\n",
    "                print(f\"Shoremasking in {loc}.\")\n",
    "\n",
    "                shore=shoremask.query(f\"location=='{loc}'\")\n",
    "                loc_selection=dataset_to_clean.query(f\"location=='{loc}'\")\n",
    "                in_shore=loc_selection[loc_selection.geometry.intersects(shore.geometry)]\n",
    "                if in_shore.shape[0]>=1:\n",
    "                    pass\n",
    "                else:\n",
    "                    in_shore=loc_selection[loc_selection.geometry.intersects(shore.to_crs(crs_dict_string[loc]).geometry.any())]\n",
    "\n",
    "                print(f\"Removing {loc_selection.shape[0] - in_shore.shape[0]} pts falling outside provided shore polygones.\")\n",
    "                inshore_cleaned=pd.concat([in_shore,inshore_cleaned], ignore_index=True)\n",
    "\n",
    "            print(f\"{processes} completed.\")\n",
    "\n",
    "            if 'watermasked_label' in inshore_cleaned.columns and 'finetuned_label' not in inshore_cleaned.columns:\n",
    "                inshore_cleaned['pt_class']=inshore_cleaned.watermasked_label\n",
    "                inshore_cleaned.drop(['watermasked_label'], axis=1, inplace=True)\n",
    "\n",
    "            elif 'finetuned_label' in inshore_cleaned.columns and 'watermasked_label' not in inshore_cleaned.columns:\n",
    "                inshore_cleaned['pt_class']=inshore_cleaned.finetuned_label\n",
    "                inshore_cleaned.drop(['finetuned_label'], axis=1, inplace=True)\n",
    "\n",
    "            elif 'finetuned_label' in inshore_cleaned.columns and 'watermasked_label' in inshore_cleaned.columns:\n",
    "                inshore_cleaned['pt_class']=inshore_cleaned.watermasked_label\n",
    "                inshore_cleaned.drop(['finetuned_label','watermasked_label'], axis=1, inplace=True)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            return inshore_cleaned\n",
    "        else:\n",
    "            raise NameError(\"shoremask file path is invalid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d9331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aef294",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_dict={'leo_20180606':[0,9,10],\n",
    "'leo_20180713':[0,3,4,7],\n",
    "'leo_20180920':[0,2,6,7],\n",
    "'leo_20190211':[0,2,5],\n",
    "'leo_20190328':[2,4,5],\n",
    "'leo_20190731':[0,2,8,6],\n",
    "'mar_20180601':[1,6],\n",
    "'mar_20180621':[4,6],\n",
    "'mar_20180727':[0,5,9,10],\n",
    "'mar_20180925':[6],\n",
    "'mar_20181113':[1],\n",
    "'mar_20181211':[4],\n",
    "'mar_20190205':[],\n",
    "'mar_20190313':[],\n",
    "'mar_20190516':[4,7]}\n",
    "\n",
    "no_sand_dict={'leo_20180606':[5],\n",
    "'leo_20180713':[],\n",
    "'leo_20180920':[],\n",
    "'leo_20190211':[1],\n",
    "'leo_20190328':[],\n",
    "'leo_20190731':[1],\n",
    "'mar_20180601':[4,5],\n",
    "'mar_20180621':[3,5],\n",
    "'mar_20180727':[4,7],\n",
    "'mar_20180925':[5],\n",
    "'mar_20181113':[0],\n",
    "'mar_20181211':[0],\n",
    "'mar_20190205':[0,5],\n",
    "'mar_20190313':[4],\n",
    "'mar_20190516':[2,5]}\n",
    "\n",
    "veg_dict={'leo_20180606':[1,3,7,8],\n",
    "'leo_20180713':[1,5,9],\n",
    "'leo_20180920':[1,4,5],\n",
    "'leo_20190211':[4],\n",
    "'leo_20190328':[0,1,6],\n",
    "'leo_20190731':[3,7],\n",
    "'mar_20180601':[0,7],\n",
    "'mar_20180621':[1,7],\n",
    "'mar_20180727':[1,3],\n",
    "'mar_20180925':[1,3],\n",
    "'mar_20181113':[3],\n",
    "'mar_20181211':[2],\n",
    "'mar_20190205':[3],\n",
    "'mar_20190313':[1,5],\n",
    "'mar_20190516':[0]}\n",
    "\n",
    "sand_dict={'leo_20180606':[2,4,6],\n",
    "'leo_20180713':[2,6,8],\n",
    "'leo_20180920':[3],\n",
    "'leo_20190211':[3],\n",
    "'leo_20190328':[3],\n",
    "'leo_20190731':[4,5],\n",
    "'mar_20180601':[2,3],\n",
    "'mar_20180621':[0,2],\n",
    "'mar_20180727':[2,6,8],\n",
    "'mar_20180925':[0,4,2],\n",
    "'mar_20181113':[2,4],\n",
    "'mar_20181211':[3,1],\n",
    "'mar_20190205':[1,2,4],\n",
    "'mar_20190313':[0,2,3],\n",
    "'mar_20190516':[1,3,6]}\n",
    "\n",
    "l_dicts={'no_sand': no_sand_dict,\n",
    "         'sand': sand_dict,\n",
    "        'water': water_dict,\n",
    "        'veg':veg_dict}# set the path to the test data folder\n",
    "test_data_folder = r\"C:\\my_packages\\sandpyper\\examples\\test_data\"\n",
    "\n",
    "# the paths to the DSM, orthophotos and transect directories\n",
    "dirNameDSM=Path(test_data_folder + r\"\\dsm_1m\")\n",
    "dirNameOrtho=Path(test_data_folder + r\"\\orthos_1m\")\n",
    "dirNameTrans=Path(test_data_folder + r\"\\transects\")\n",
    "\n",
    "# path to the LoD transects\n",
    "\n",
    "lod_mode=Path(test_data_folder + r\"\\lod_transects\")\n",
    "\n",
    "\n",
    "# the location codes used for the monitored locations\n",
    "loc_codes=[\"mar\",\"leo\"]\n",
    "\n",
    "\n",
    "# the keyword search dictionary\n",
    "loc_search_dict = {   'leo': ['St','Leonards','leonards','leo'],\n",
    "                   'mar': ['Marengo','marengo','mar'] }\n",
    "\n",
    "\n",
    "# the EPSG codes of the coordinate reference systems for each location code (location) given in CRS string format\n",
    "crs_dict_string= {\n",
    "                 'mar': {'init': 'epsg:32754'},\n",
    "                 'leo':{'init': 'epsg:32755'}\n",
    "                 }\n",
    "\n",
    "# the transect spacing of the transects\n",
    "transects_spacing=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c44d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "P=ProfileSet(dirNameDSM=dirNameDSM,\n",
    "            dirNameOrtho=dirNameOrtho,\n",
    "            dirNameTrans=dirNameTrans,\n",
    "            transects_spacing=transects_spacing,\n",
    "            loc_codes=loc_codes,\n",
    "            loc_search_dict=loc_search_dict,\n",
    "            crs_dict_string=crs_dict_string,\n",
    "            check=\"all\")\n",
    "\n",
    "# run extraction from DSMs and orthos with 1m sampling steps and add X and Y fields to output geodataframe.\n",
    "# use LoDs profiles provided.\n",
    "\n",
    "P.extract_profiles(mode='all',tr_ids='tr_id',sampling_step=1,add_xy=True,lod_mode=lod_mode)\n",
    "\n",
    "# Run interatively KMeans + SA using the feature_set provided\n",
    "#feel free to add \n",
    "\n",
    "feature_set=[\"band1\",\"band2\",\"band3\",\"distance\"]\n",
    "sil_df=get_sil_location(P.profiles,\n",
    "                        ks=(2,15), \n",
    "                        feature_set=feature_set,\n",
    "                       random_state=10)\n",
    "\n",
    "opt_k=get_opt_k(sil_df, sigma=0 )\n",
    "opt_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_k_broke={'leo_20180606': 10,\n",
    " 'leo_20180713': 10,\n",
    " 'leo_20180920': 10,\n",
    " 'leo_20190211': 10,\n",
    " 'leo_20190328': 10,\n",
    " 'leo_20190731': 10,\n",
    " 'mar_20180601': 10,\n",
    " 'mar_20180621': 10,\n",
    " 'mar_20180727': 10,\n",
    " 'mar_20180925': 10,\n",
    " 'mar_20181113': 10,\n",
    " 'mar_20181211': 10,\n",
    " 'mar_20190205': 10,\n",
    " 'mar_20190313': 10,\n",
    " 'mar_20190516': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c84e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profiles=pd.read_csv(r\"C:\\my_packages\\tmp\\check_cleanit.csv\")\n",
    "profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b43210",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set=[\"band1\",\"band2\",\"band3\",\"distance\"]\n",
    "P.kmeans_sa(opt_k, feature_set=feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set=[\"band1\",\"band2\",\"band3\",\"distance\"]\n",
    "\n",
    "prof_works=P.kmeans_sa(a, ks=opt_k,feature_set=feature_set )\n",
    "#prof_broke=kmeans_sa(profiles, ks=opt_k_broke,feature_set=feature_set )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be003ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sandpyper.common import coords_to_points\n",
    "from shapely.geometry.point import Point\n",
    "\n",
    "test=P.profiles.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e917b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_corrections_path=Path(test_data_folder + r\"\\clean\\label_corrections.gpkg\")\n",
    "watermasks_path=Path(test_data_folder + r\"\\clean\\watermasks.gpkg\")\n",
    "shoremasks_path=Path(test_data_folder + r\"\\clean\\shoremasks.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reclassifying dataset with the provided dictionaries.\" )\n",
    "to_clean_classified=classify_labelk(test, l_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_corrections=gpd.read_file(label_corrections_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738771f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleanit(test, l_dicts, cluster_field='label_k', fill_class='sand',\n",
    "                watermasks_path=watermasks_path, water_label='water',\n",
    "                shoremasks_path=shoremasks_path, label_corrections_path=label_corrections_path,\n",
    "                crs_dict_string=crs_dict_string,\n",
    "               geometry_field='coordinates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61862013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27521a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab9ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26282645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7c08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491b4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe620e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots()\n",
    "pts_gdf.plot(ax=ax)\n",
    "intersection_gdf.boundary.plot(ax=ax, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_corrections=label_corrections\n",
    "profiles=to_clean_classified\n",
    "crs=crs_dict_string\n",
    "\n",
    "def check_overlaps_poly_label(label_corrections, profiles,crs):\n",
    "    \"\"\"\n",
    "    Function to check wether overlapping areas of label correction polygons targeting the same label_k in the same surveys but assigning different new classes do not contain points that would be affected by those polygons.\n",
    "    Args:\n",
    "        label_corrections (gpd.GeoDataFrame): GeodataFrame of the label correction polygons.\n",
    "        profiles (gpd.GeoDataFrame): Geodataframe of the extracted elevation profiles.\n",
    "        crs (dict, int): Either an EPSG code (int) or a dictionary. If dictionary, it must store location codes as keys and crs information as values, in dictionary form (example: {'init' :'epsg:4326'}).\n",
    "\n",
    "    \"\"\"\n",
    "    for loc in label_corrections.location.unique():\n",
    "        for raw_date in label_corrections.query(f\"location=='{loc}'\").raw_date.unique():\n",
    "            for target_label_k in label_corrections.query(f\"location=='{loc}' and raw_date=={raw_date}\").target_label_k.unique():\n",
    "\n",
    "                date_labelk_subset=label_corrections.query(f\"location=='{loc}' and raw_date=={raw_date} and target_label_k=={int(target_label_k)}\")\n",
    "\n",
    "                # if more than one polygons target the same label k, check if they overlap\n",
    "                if len(date_labelk_subset)>1:\n",
    "\n",
    "                    # check if there are any one of them that overlaps\n",
    "                    for i,z in comb(range(len(date_labelk_subset)),2):\n",
    "                        intersection_gdf = overlay(date_labelk_subset.iloc[[i]], date_labelk_subset.iloc[[z]], how='intersection')\n",
    "\n",
    "                        if not intersection_gdf.empty:\n",
    "\n",
    "                           # check if the overlapping polygons have assigns different new_classes\n",
    "                            if any(intersection_gdf.new_class_1 != intersection_gdf.new_class_2):\n",
    "\n",
    "                                # if overlap areas assign different classes, check if this area contains points with label_k equal to both polygons target_label_k..\n",
    "                                # if contains points, raise an error as it does not make sense and the polygons must be corrected\n",
    "                                # by the user\n",
    "\n",
    "                                pts=profiles.query(f\"location=='{loc}' and raw_date=={raw_date} and label_k=={int(target_label_k)}\")\n",
    "\n",
    "                                if isinstance(pts.iloc[0]['coordinates'],Point):\n",
    "                                    pts_gdf=pts\n",
    "                                elif isinstance(pts.iloc[0]['coordinates'],str):\n",
    "                                    pts['coordinates']=pts.coordinates.apply(coords_to_points)\n",
    "                                    if isinstance(crs, dict):\n",
    "                                        pts_gdf=gpd.GeoDataFrame(pts, geometry='coordinates', crs=crs[loc])\n",
    "                                    elif isinstance(crs, int):\n",
    "                                        crs_adhoc={'init': f'epsg:{crs}'}\n",
    "                                        pts_gdf=gpd.GeoDataFrame(pts, geometry='coordinates', crs=crs_adhoc)\n",
    "                                else:\n",
    "                                    raise ValueError(f\"profiles coordinates field must contain points coordinates either as Shapely Point geometry objects or as a string representing a Shapely Point geometry in well known text. Found {type(pts.iloc[0]['coordinates'])} type instead.\")\n",
    "\n",
    "                                fully_contains = [intersection_gdf.geometry.contains(mask_geom)[0] for mask_geom in pts_gdf.geometry]\n",
    "\n",
    "                                if True in fully_contains:\n",
    "                                    idx_true=[i for i, x in enumerate(fully_contains) if x]\n",
    "                                    raise ValueError(f\"There are {len(intersection_gdf)} points in the overlap area of two label correction polygons (location: {loc}, raw_date: {raw_date}, target_label_k = {target_label_k}) which assign two different classes: {intersection_gdf.loc[:,'new_class_1'][0], intersection_gdf.loc[:,'new_class_2'][0]}. This doesn't make sense, please correct your label correction polygons. You can have overlapping polygons which act on the same target label k, but if they overlap points with such target_label_k, then they MUST assign the same new class.\")\n",
    "\n",
    "    print(\"Check label correction polygons overlap inconsistencies terminated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16daac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_overlaps_poly_label(label_corrections=label_corrections,profiles=to_clean_classified,crs=crs_dict_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c33940",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25947660",
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in label_corrections.location.unique():\n",
    "    for raw_date in label_corrections.query(f\"location=='{loc}'\").raw_date.unique():\n",
    "        for target_label_k in label_corrections.query(f\"location=='{loc}' and raw_date=={raw_date}\").target_label_k.unique():\n",
    "            pts=P.profiles.query(f\"location=='{loc}' and raw_date=={raw_date} and label_k=={int(target_label_k)}\")\n",
    "            print(pts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a414bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5769e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776652f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry.point import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e943ba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test=check_overlaps_poly_label(label_corrections,prof_works, crs=crs_dict_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d59e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c299ad23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcf9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pts_gdf.iloc[0]['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f46e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(pts_gdf.iloc[0]['location'],str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fec3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots()\n",
    "\n",
    "#to_clean_classified.query(\"point_id=='3300o21753081509le201'\").plot(ax=ax)\n",
    "#subset_finetune_polys[-2:].plot(column='index', alpha=.3, ax=ax)\n",
    "intersection_gdf.reset_index().plot(ax=ax, column='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901517fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_update_finetune.query(\"point_id=='3300o21753081509le201'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_dict_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d212e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d71b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
