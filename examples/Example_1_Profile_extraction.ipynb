{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><center> <b>Sandpyper: sandy beaches SfM-UAV analysis tools</b></center></font>\n",
    "<font size=\"4\"><center> <b> Example 1 - Profiles extraction </b></center> <br>\n",
    "\n",
    "    \n",
    "<center><img src=\"images/banner.png\" width=\"80%\"  /></center>\n",
    "\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"5\"> <b>Profiles creation and data extraction from DSM and orthophotos</b></font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Nicolas Pucino; PhD Student @ Deakin University, Australia </b> <br>\n",
    "\n",
    "<font size=\"3\">The first steps in a typical workflow is to create cross-shore transects in all the locations and extract elevation and RGB information along those transects. Sandpiper allows the data extraction from hundreds of rasters at once, in an organised way. <br>\n",
    "\n",
    "<b>This notebook covers the following concepts:</b>\n",
    "\n",
    "- Naming conventions and global parameters.\n",
    "- Setting up the folders.\n",
    "- Setting up the folders.\n",
    "</font>\n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all it is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "from sandpyper.outils import cross_ref\n",
    "from sandpyper.profile import extract_from_folder\n",
    "from sandpyper.space import create_transects\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfileSet():\n",
    "    \"\"\"\n",
    "    ciao\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dirNameDSM,\n",
    "                 dirNameOrtho,\n",
    "                 dirNameTrans,\n",
    "                 loc_codes,\n",
    "                 loc_search_dict,\n",
    "                 crs_dict_string,\n",
    "                check='all'):\n",
    "        \n",
    "        \n",
    "        self.dirNameDSM=dirNameDSM\n",
    "        self.dirNameOrtho=dirNameOrtho\n",
    "        self.dirNameTrans=dirNameTrans\n",
    "        \n",
    "        self.loc_codes=loc_codes\n",
    "        self.loc_search_dict=loc_search_dict\n",
    "        self.crs_dict_string=crs_dict_string\n",
    "        \n",
    "        if check==\"dsm\":\n",
    "            path_in=self.dirNameDSM\n",
    "        elif check == \"ortho\":\n",
    "            path_in=self.dirNameOrtho\n",
    "        elif check == \"all\":\n",
    "            path_in=[self.dirNameDSM, self.dirNameOrtho]\n",
    "            \n",
    "        \n",
    "        self.check=cross_ref(path_in,\n",
    "                        self.dirNameTrans,\n",
    "                        print_info=True, \n",
    "                        loc_search_dict=self.loc_search_dict,\n",
    "                        list_loc_codes=self.loc_codes)\n",
    "\n",
    "        \n",
    "    def extract_profiles(self,\n",
    "                         mode,\n",
    "                         sampling_step,\n",
    "                         add_xy,\n",
    "                         add_slope=False,\n",
    "                         default_nan_values=-10000):\n",
    "        \n",
    "        if mode==\"dsm\":\n",
    "            path_in=self.dirNameDSM\n",
    "        elif mode == \"ortho\":\n",
    "            path_in=self.dirNameOrtho\n",
    "        elif mode == \"all\":\n",
    "            path_in=[self.dirNameDSM,self.dirNameOrtho]\n",
    "        else:\n",
    "            raise NameError(\"mode must be either 'dsm','ortho' or 'all'.\")\n",
    "        \n",
    "        if mode in [\"dsm\",\"ortho\"]:\n",
    "            \n",
    "            profiles=extract_from_folder(dataset_folder=path_in,\n",
    "                transect_folder=self.dirNameTrans,\n",
    "                mode=mode,sampling_step=sampling_step,\n",
    "                list_loc_codes=self.loc_codes,\n",
    "                add_xy=add_xy,\n",
    "                add_slope=add_slope,\n",
    "                default_nan_values=default_nan_values)\n",
    "        \n",
    "            if mode==\"dsm\":\n",
    "\n",
    "                self.profiles_z=profiles\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.profiles_rgb=profiles\n",
    "            \n",
    "        elif mode == \"all\":\n",
    "            \n",
    "            profiles_z=extract_from_folder( dataset_folder=path_in[0],\n",
    "                    transect_folder=self.dirNameTrans,\n",
    "                    mode=mode,\n",
    "                    sampling_step=sampling_step,\n",
    "                    list_loc_codes=self.loc_codes,\n",
    "                    add_xy=add_xy,\n",
    "                    add_slope=add_slope,\n",
    "                    default_nan_values=default_nan_values )\n",
    "            \n",
    "            self.profiles_z=profiles_z\n",
    "            \n",
    "            profiles_rgb=extract_from_folder(dataset_folder=path_in[1],\n",
    "                transect_folder=self.dirNameTrans,\n",
    "                mode=mode,sampling_step=sampling_step,\n",
    "                list_loc_codes=self.loc_codes,\n",
    "                add_xy=add_xy,\n",
    "                default_nan_values=default_nan_values)\n",
    "            \n",
    "            self.profiles_rgb=profiles_rgb\n",
    "        \n",
    "        else:\n",
    "            raise NameError(\"mode must be either 'dsm','ortho' or 'all'.\")\n",
    "        \n",
    "        self.sampling_step=sampling_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirNameDSM=r'C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1m'\n",
    "\n",
    "dirNameOrtho=r'C:\\my_packages\\sandpyper\\tests\\test_data\\orthos_1m'\n",
    "\n",
    "dirNameTrans=r'C:\\my_packages\\sandpyper\\tests\\test_data\\transects'\n",
    "\n",
    "loc_codes=[\"mar\",\"leo\"]\n",
    "loc_search_dict = {   'leo': ['St','Leonards','leonards','leo'],\n",
    "                      'mar': ['Marengo','marengo','mar'] }\n",
    "crs_dict_string= {\n",
    "                 'mar': {'init': 'epsg:32754'},\n",
    "                 'leo':{'init': 'epsg:32755'}\n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\geopandas\\geodataframe.py:422: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for feature in features_lst:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsm from leo = 6\n",
      "\n",
      "ortho from leo = 6\n",
      "\n",
      "dsm from mar = 9\n",
      "\n",
      "ortho from mar = 9\n",
      "\n",
      "\n",
      "NUMBER OF DATASETS TO PROCESS: 30\n"
     ]
    }
   ],
   "source": [
    "P=ProfileSet(dirNameDSM,\n",
    "            dirNameOrtho,\n",
    "            dirNameTrans,\n",
    "            loc_codes,\n",
    "            loc_search_dict,\n",
    "            crs_dict_string,\n",
    "            check=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d4f95875c647dc8e9c3315d0c65763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\geopandas\\geodataframe.py:422: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for feature in features_lst:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2676757d22448309db769f0b7eaa64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85d95ab49f54190886f74b0b47dbe7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b462100a88bd4db7b1a8394f29262e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b6a9db9fba4d819a1e66dfe1f3c3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592d3080b56b416e93cebdf42e7cbdc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1476f5df74264a0f80951573ccd5cee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8673471022464338a10699c04aadd5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b84171b94e48e282a37a30fbc293df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f86560f77445d2a99f1fbc669f3d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca24cbf08e64cc98ee51dcbefde51ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c5e6f9cfd745a1935b6319811ee1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91f2fc472cc4eeda808b964bbc8c7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d35233fd4049f385b56b0cbc3ce6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a91633e2a51406c8f69b61c8032a79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9805f4179d4fcda7ec9397804e16fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction succesfull\n",
      "Number of points extracted:741\n",
      "Time for processing=17.538551092147827 seconds\n",
      "First 10 rows are printed below\n",
      "Number of points outside the raster extents: 180\n",
      "The extraction assigns NaN.\n",
      "Number of points in NoData areas within the raster extents: 22\n",
      "The extraction assigns NaN.\n"
     ]
    }
   ],
   "source": [
    "P.extract_profiles('dsm',50,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P.profiles_z.query(\"location=='mar'\").plot(column=\"z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global parameters\n",
    "### Location codes\n",
    "When your analysis involves a multi-site approach, it is convenient to assign each location a __small code__ to easy the handling of every associated file and its coordinate reference system.\n",
    "\n",
    "Here are some examples:\n",
    "* Saint Leonards : __leo__\n",
    "* Marengo : __mar__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location codes used troughout the analysis\n",
    "loc_codes=[\"mar\",\"leo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location search dictionary\n",
    "Sometimes, we need to automatically obtain the right location code from raster files, either of Digital Surface Models (DSM) or Orthophotos (ORTO), whose filenames contains the original location name (e.g. Saint_Leonards or Warrnambool).\n",
    "\n",
    "An easy and fast way to do this, is to create a dictionary, where __keys are the location codes__ and the __values are lists of possible full names__ we expect to find in the files.\n",
    "Here are some examples:\n",
    "\n",
    "```python\n",
    "loc_search_dict = {   'leo': ['St','Leonards','leonards','leo'],\n",
    "                      'mar': ['Marengo','marengo','mar'] }\n",
    "```\n",
    "> __NOTE__: always include the location codes in the list of possible names, in case the original raster filenames are already formatted!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The terms used in the original filenames.\n",
    "# These will be used to properly format files, extracting location codes and dates.\n",
    "\n",
    "loc_search_dict = {   'leo': ['St','Leonards','leonards','leo'],\n",
    "                      'mar': ['Marengo','marengo','mar'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Reference Systems dictionary\n",
    "\n",
    "Working on a wide area, often requires dealing with multiple Coordinate Reference Systems (CRS). Therefore, it is important to assign __each location code with its appropriate CRS__ at the beginning, in order to always take it into account trhoughout the analysis.\n",
    "\n",
    "We do this with another dictionary, called `crs_dict_string`, where as keys we store the location codes and as values we store another dictionary, in this form `{'init': 'epsg:32754'}` .\n",
    "Modify the __EPSG code__ to change CRS. Here is an example of the resulting dictionary:\n",
    "\n",
    "```python\n",
    "crs_dict_string = {'wbl': {'init': 'epsg:32754'},\n",
    "                   'apo': {'init': 'epsg:32754'},\n",
    "                   'prd': {'init': 'epsg:32755'},\n",
    "                   'dem': {'init': 'epsg:32755'} }\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "> __NOTE:__ to specify the CRS, use the dictionary format supported by Geopandas 0.6.3. Only projected CRS are supported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Coordinate Reference Systems used troughout this example\n",
    "\n",
    "crs_dict_string= {\n",
    "                 'mar': {'init': 'epsg:32754'},\n",
    "                 'leo':{'init': 'epsg:32755'}\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transects creation\n",
    "### Shoreline baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any Shapely line or polyline object can be used as input as a transect. In coastal geomeorphometric studies, transects are usually equally spaced alongshore, and place normal to the shoreline. However, any type of line can be used to extract values from both orthophtos and DSMs.\n",
    "\n",
    "You can construct transects in 2 ways:\n",
    "1. **Any GIS**\n",
    "2. Using the function **create_transects**, starting from a __shoreline baseline__.\n",
    "\n",
    "If you use your favourite GIS (Qgis preferred), ensure that the output format is __geopackage (.gpkg)__, and:\n",
    "> **each transect must be in a separate row (geometry)**\n",
    "\n",
    "If you want to use in-built sandpyper function, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and display the shoreline\n",
    "path_to_shoreline_mar=r'C:\\my_packages\\doc_data\\test_data\\shorelines\\leo_shoreline_short.gpkg' # Marengo shoreline\n",
    "\n",
    "shoreline=gpd.read_file(path_to_shoreline_mar)\n",
    "shoreline.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and display the transects (in red)\n",
    "\n",
    "f,ax=plt.subplots(figsize=(10,10))  # Change figsize if you want bigger images\n",
    "location='mar'  # insert the location code for this transect.\n",
    "\n",
    "transects=create_transects(shoreline,\n",
    "                           sampling_step=20, # alongshore spacing\n",
    "                           tick_length=50, # transects length\n",
    "                           location=location,crs=crs_dict_string[location],\n",
    "                           side='both' # 'both':transect is centered at the interesction with the baseline\n",
    "                          )\n",
    "\n",
    "# Modify the figure by plotting shoreline, transects and transect IDs.\n",
    "shoreline.plot(ax=ax,color='b')\n",
    "transects.plot(ax=ax,color='r')\n",
    "\n",
    "for x, y, label in zip(transects.geometry.centroid.x, transects.geometry.centroid.y, transects.tr_id):\n",
    "    ax.annotate(label, xy=(x, y), xytext=(4, 1), textcoords=\"offset points\")\n",
    "\n",
    "transects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell saves transect to file and name it as:\n",
    "\n",
    ">__locationCode_whateverYouWant.gpkg__\n",
    "\n",
    "(example: __leo_transects.gpkg__)\n",
    "\n",
    "and place it to a folder where you store all the transects for each location.\n",
    "\n",
    ">__Note:__ If the saving throws an error \"PLE_NotSupported in dataset leo_transects.gpkg does not support layer creation option ENCODING\", the file should be created and valid anyway. Double-check by opening it in Qgis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transects.to_file(filename=r'C:\\my_packages\\doc_data\\transects\\leo_transects.gpkg',driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elevation and RGB data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the folders containing the datasets\n",
    "\n",
    "First, let's define the paths to the folders containing the raster DSMs or ORTOs and the transects.\n",
    "\n",
    "> __SUPPORTED FORMATS:__\n",
    ">* Rasters: __geotiffs (.tif, .tiff)__\n",
    ">* Transects: __geopackages (.gpkg).__\n",
    ">\n",
    ">__Transects filenames:__ when creating transects, save them with the location code as filename."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hill_ortho_profiles_cow.png\" width=\"85%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the path to the folders containing the DSMs (dirNameDSM) and the transect files (dirNameTrans)\n",
    "\n",
    "dirNameDSM=r'C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1m'\n",
    "\n",
    "dirNameOrtho=r'C:\\my_packages\\sandpyper\\tests\\test_data\\orthos_1m'\n",
    "\n",
    "dirNameTrans=r'C:\\my_packages\\sandpyper\\tests\\test_data\\transects'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check cross-reference table and CRS matches\n",
    "\n",
    "The EPSG codes in the raster and transect columns __must match__.\n",
    "\n",
    "Please also check the dsm and date columns match transect file and dsm file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check=cross_ref(dirNameDSM,dirNameTrans,print_info=True, loc_search_dict=loc_search_dict,list_loc_codes=loc_codes)\n",
    "check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We are now ready to extract data from DSMs or ORTOs with the provided transect files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of profiles from folder\n",
    "\n",
    "This is the cell where the automatic extraction gets processed.\n",
    "The only parameter to set is the __sampling step__ variable, which indicates the __cross-shore sampling distance (m)__ that we want to use along our transects. Beware, although you could use a very small sampling distance (UAV datasets tend to be between few to 10 cm pixel size), file dimension will increase significantly!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dealing with NaNs__\n",
    "\n",
    ">NaNs might come from two different cases:\n",
    ">1. extraction of points generated on transects falling __outside__ of the underlying raster extent\n",
    ">2. points sampled from transect __inside__ the raster extent but containing NoData cells.\n",
    ">\n",
    ">Conveniently, the extraction profile function makes sure that if points fall outside the raster extent (case 1), those >elevations are assigned a default nan value, in the NumPy np.nan form.\n",
    ">In case 2, however, the values extracted depends on the definition of NaNs of the source raster format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Parameters to specify\n",
    "\n",
    "transect_folder=dirNameTrans\n",
    "sampling_step=1\n",
    "\n",
    "\n",
    "gdf_rgb=extract_from_folder(dataset_folder=dirNameOrtho,\n",
    "                        transect_folder=transect_folder,\n",
    "                        mode=\"ortho\",sampling_step=sampling_step,\n",
    "                        list_loc_codes=loc_codes,\n",
    "                        add_xy=True)\n",
    "\n",
    "gdf=extract_from_folder(dataset_folder=dirNameDSM,\n",
    "                        transect_folder=transect_folder,\n",
    "                        mode=\"dsm\",sampling_step=sampling_step,\n",
    "                        list_loc_codes=loc_codes,\n",
    "                        add_xy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOD!\n",
    "\n",
    "save the Geodataframes (gdf and gdf_rgb) as a CSV file and head to the __SANDPYPER Labeling sand notebook__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the files\n",
    "\n",
    "gdf.to_csv(r\"C:\\my_packages\\sandpyper\\tests\\test_outputs\\gdf.csv\",index=False)\n",
    "gdf_rgb.to_csv(r\"C:\\my_packages\\sandpyper\\tests\\test_outputs\\gdf_rgb.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
