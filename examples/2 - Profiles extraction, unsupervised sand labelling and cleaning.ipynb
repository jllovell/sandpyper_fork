{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca8c18a",
   "metadata": {},
   "source": [
    "<font size=\"5\"><center> <b>Sandpyper: sandy beaches SfM-UAV analysis tools</b></center></font>\n",
    "\n",
    "    \n",
    "<center><img src=\"images/banner.png\" width=\"80%\"  /></center>\n",
    "\n",
    "<font face=\"Calibri\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f277f",
   "metadata": {},
   "source": [
    "# Profile extraction, LoD and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee748d",
   "metadata": {},
   "source": [
    "<font size=\"3\"> <b> Nicolas Pucino; PhD Student @ Deakin University, Australia </b> <br>\n",
    "\n",
    "<b>This notebook covers the following concepts:</b>\n",
    "\n",
    "- The ProfileSet class.\n",
    "- Data extraction.\n",
    "- Limit of Detections (LoDs).\n",
    "</font>\n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d2c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\pysal\\explore\\segregation\\network\\network.py:15: UserWarning: You need pandana and urbanaccess to work with segregation's network module\n",
      "You can install them with  `pip install urbanaccess pandana` or `conda install -c udst pandana urbanaccess`\n",
      "  warn(\n",
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\pysal\\model\\spvcm\\abstracts.py:10: UserWarning: The `dill` module is required to use the sqlite backend fully.\n",
      "  from .sqlite import head_to_sql, start_sql\n",
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rasterio as ras\n",
    "from rasterio.plot import show\n",
    "\n",
    "from sandpyper.sandpyper import ProfileSet\n",
    "from sandpyper.common import get_sil_location, get_opt_k\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf4119",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ee8a3",
   "metadata": {},
   "source": [
    "The __ProfileSet__ class is an object that contains all the information necessary to carry a typical *sandpyper* analysis from start to end. Once instantiated, it will contains a number of global variables that store some fundamental monitoring charasteristics such as:\n",
    "\n",
    "* paths to the data directories\n",
    "* coordinate reference systems of each site\n",
    "* location codes and search keywords\n",
    "* some monitoring parameters such as alongshore transect spacing, acrosshore sampling step and cleaning steps used\n",
    "* and of course, the elevation and colour profile data.\n",
    "\n",
    "Moreover, a few key methods will:\n",
    "\n",
    "1. extract data from the provided transects\n",
    "2. cluster extracted points using unsupervised clustering algorithm KMeans\n",
    "3. clean the dataset with user-provided watermasks, shoremasks and fine-tuning polygons.\n",
    "\n",
    "Moreover, when the __ProfileSet__ class is instantiated, it prints out the number of DSMs and orthophotos found for each location in the provided folders and it also creates a dataframe storing filenames, CRSs, location codes and raw dates extracted from the filenames. This dataframe is useful as a sanity check to see if all the information are correct before proceeding with the actual profile extraction. It will be stored as the attribute *ProfileSet.check*.\n",
    "\n",
    "Let's see here how it all works...\n",
    "\n",
    "First, create the required monitoring global settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6445d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the paths to the DSM, orthophotos and transect directories\n",
    "dirNameDSM=r'C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1m'\n",
    "dirNameOrtho=r'C:\\my_packages\\sandpyper\\tests\\test_data\\orthos_1m'\n",
    "dirNameTrans=r'C:\\my_packages\\sandpyper\\tests\\test_data\\transects'\n",
    "\n",
    "# the location codes used for the monitored locations\n",
    "loc_codes=[\"mar\",\"leo\"]\n",
    "\n",
    "\n",
    "# the keyword search dictionary\n",
    "loc_search_dict = {   'leo': ['St','Leonards','leonards','leo'],\n",
    "                   'mar': ['Marengo','marengo','mar'] }\n",
    "\n",
    "\n",
    "# the EPSG codes of the coordinate reference systems for each location code (location) given in CRS string format\n",
    "crs_dict_string= {\n",
    "                 'mar': {'init': 'epsg:32754'},\n",
    "                 'leo':{'init': 'epsg:32755'}\n",
    "                 }\n",
    "\n",
    "# the transect spacing of the transects\n",
    "transects_spacing=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707dea5f",
   "metadata": {},
   "source": [
    "Then, create an instance of the ProfileSet class. Let's set 'check' parameter to 'all' to create the check dataframe\n",
    "and store it in the variable P.check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39067452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\geopandas\\geodataframe.py:422: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for feature in features_lst:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsm from leo = 6\n",
      "\n",
      "ortho from leo = 6\n",
      "\n",
      "dsm from mar = 9\n",
      "\n",
      "ortho from mar = 9\n",
      "\n",
      "\n",
      "NUMBER OF DATASETS TO PROCESS: 30\n"
     ]
    }
   ],
   "source": [
    "P=ProfileSet(dirNameDSM=dirNameDSM,\n",
    "            dirNameOrtho=dirNameOrtho,\n",
    "            dirNameTrans=dirNameTrans,\n",
    "            transects_spacing=transects_spacing,\n",
    "            loc_codes=loc_codes,\n",
    "            loc_search_dict=loc_search_dict,\n",
    "            crs_dict_string=crs_dict_string,\n",
    "            check=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e738d0",
   "metadata": {},
   "source": [
    "Check the infromation extracted from the formatted filenames.\n",
    "All the CRSs in a line (survey) must match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35fd432f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_date</th>\n",
       "      <th>location</th>\n",
       "      <th>filename_trs_ortho</th>\n",
       "      <th>crs_transect_dsm</th>\n",
       "      <th>filename_raster_dsm</th>\n",
       "      <th>filename_raster_ortho</th>\n",
       "      <th>crs_raster_dsm</th>\n",
       "      <th>crs_raster_ortho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180606</td>\n",
       "      <td>leo</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32755</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32755</td>\n",
       "      <td>32755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180713</td>\n",
       "      <td>leo</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32755</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32755</td>\n",
       "      <td>32755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180920</td>\n",
       "      <td>leo</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32755</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32755</td>\n",
       "      <td>32755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190211</td>\n",
       "      <td>leo</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32755</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32755</td>\n",
       "      <td>32755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190328</td>\n",
       "      <td>leo</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32755</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32755</td>\n",
       "      <td>32755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20190731</td>\n",
       "      <td>leo</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32755</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32755</td>\n",
       "      <td>32755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20180601</td>\n",
       "      <td>mar</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32754</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32754</td>\n",
       "      <td>32754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20180621</td>\n",
       "      <td>mar</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32754</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32754</td>\n",
       "      <td>32754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20180727</td>\n",
       "      <td>mar</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32754</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32754</td>\n",
       "      <td>32754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20180925</td>\n",
       "      <td>mar</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32754</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32754</td>\n",
       "      <td>32754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20181113</td>\n",
       "      <td>mar</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32754</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32754</td>\n",
       "      <td>32754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20181211</td>\n",
       "      <td>mar</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32754</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32754</td>\n",
       "      <td>32754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20190205</td>\n",
       "      <td>mar</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32754</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32754</td>\n",
       "      <td>32754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20190313</td>\n",
       "      <td>mar</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32754</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32754</td>\n",
       "      <td>32754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20190516</td>\n",
       "      <td>mar</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\trans...</td>\n",
       "      <td>epsg:32754</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...</td>\n",
       "      <td>C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...</td>\n",
       "      <td>32754</td>\n",
       "      <td>32754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    raw_date location                                 filename_trs_ortho  \\\n",
       "0   20180606      leo  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "1   20180713      leo  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "2   20180920      leo  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "3   20190211      leo  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "4   20190328      leo  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "5   20190731      leo  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "6   20180601      mar  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "7   20180621      mar  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "8   20180727      mar  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "9   20180925      mar  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "10  20181113      mar  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "11  20181211      mar  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "12  20190205      mar  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "13  20190313      mar  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "14  20190516      mar  C:\\my_packages\\sandpyper\\tests\\test_data\\trans...   \n",
       "\n",
       "   crs_transect_dsm                                filename_raster_dsm  \\\n",
       "0        epsg:32755  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "1        epsg:32755  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "2        epsg:32755  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "3        epsg:32755  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "4        epsg:32755  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "5        epsg:32755  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "6        epsg:32754  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "7        epsg:32754  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "8        epsg:32754  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "9        epsg:32754  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "10       epsg:32754  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "11       epsg:32754  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "12       epsg:32754  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "13       epsg:32754  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "14       epsg:32754  C:\\my_packages\\sandpyper\\tests\\test_data\\dsm_1...   \n",
       "\n",
       "                                filename_raster_ortho  crs_raster_dsm  \\\n",
       "0   C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32755   \n",
       "1   C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32755   \n",
       "2   C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32755   \n",
       "3   C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32755   \n",
       "4   C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32755   \n",
       "5   C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32755   \n",
       "6   C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32754   \n",
       "7   C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32754   \n",
       "8   C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32754   \n",
       "9   C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32754   \n",
       "10  C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32754   \n",
       "11  C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32754   \n",
       "12  C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32754   \n",
       "13  C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32754   \n",
       "14  C:\\my_packages\\sandpyper\\tests\\test_data\\ortho...           32754   \n",
       "\n",
       "    crs_raster_ortho  \n",
       "0              32755  \n",
       "1              32755  \n",
       "2              32755  \n",
       "3              32755  \n",
       "4              32755  \n",
       "5              32755  \n",
       "6              32754  \n",
       "7              32754  \n",
       "8              32754  \n",
       "9              32754  \n",
       "10             32754  \n",
       "11             32754  \n",
       "12             32754  \n",
       "13             32754  \n",
       "14             32754  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d66e350",
   "metadata": {},
   "source": [
    "## Extraction of profiles from folder\n",
    "\n",
    "After checking the check dataframe and all looks right, we are ready to actually extract the data from both DSMs and orhtophotos using the *.extract_profiles* method.\n",
    "Importantly, we also extract the data for the Limit of Detections (LoD) computation, using some transects digitised over areas we expect __NOT TO BE CHANGING__ in elevation during the monitoring period, also known as calibration zones.\n",
    "\n",
    "Thus, the most important parameters to set are:\n",
    "\n",
    "1. __sampling step__: indicates the __cross-shore sampling distance (m)__ that we want to use along our transects. Beware, although you could use a very small sampling distance (UAV datasets tend to be between few to 10 cm pixel size), file dimension will increase significantly!.\n",
    "\n",
    "2. __lod__: this can be the path to a directory containing the lod transects to use as lod, a number to use as lod for all surveys in all locations or None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01a5b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the LoD transects\n",
    "\n",
    "lod_mode=r\"C:\\my_packages\\sandpyper\\tests\\test_data\\lod_transects\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05de363",
   "metadata": {},
   "source": [
    "The parameter __mode__ is here set to 'all' as we want to extract pixel values from both the DSMs and orthophotos, while the parameter __tr_ids__ specifies the name of the field in the transect file is used to store the transect IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "032dde91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting elevation from DSMs . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061f2e0a90bd4514a24cf0ef3b120c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\geopandas\\geodataframe.py:422: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for feature in features_lst:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a1f4a1334542e3b9560b127c674c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f9abafb2444452810427fd5f801428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a158a011d0406e883edbd84e10b169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b0703d3f074b2ea00d111a0206ebec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82b52758d394442adf5f3463a3a7dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37c490b669e4d60a961d622980c244e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3272fff9b7b344b087f6e09e7a31be0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531f5bdbd3e74d3598a3c7e5ff5b183d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09152e9446994b3bab0b0e48d3759b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982c95cd12e645fea154f4c30d9ee500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd8f011bc424041b833b3ae59d9c8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c474551a88334a31b43569f26229d74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e08176cde704c01985636b6660fe40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db002227e9b4501ba25225f60d1661e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b9ec9edb5941cb93c3c01094639bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction succesfull\n",
      "Number of points extracted:32805\n",
      "Time for processing=46.358020544052124 seconds\n",
      "First 10 rows are printed below\n",
      "Number of points outside the raster extents: 9066\n",
      "The extraction assigns NaN.\n",
      "Number of points in NoData areas within the raster extents: 250\n",
      "The extraction assigns NaN.\n",
      "Extracting rgb values from orthos . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed02e9246664941ab011763fa46ceaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\geopandas\\geodataframe.py:422: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for feature in features_lst:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7a0f50781449a4bca36123ba314895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c5edaf4344487b903a9185c4fdd7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878bc58ca43e429985d98b4b5e311c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274869a283ef416596d86ea1a95caac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2e0810e3d64dc39952bb55f9387497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c4282853614797bf414b4c5477794d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd8dde7a72e45159a12c3c9613f82de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadb7eabb2a04dbc9bf7e4818c5cbab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9343acb8186c4e61a3ace5d7305c5e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b218a7967274301a4c60b7b16d3753e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0a344deea4420c9c0eeb08867eb92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b4f061fad34e329ed8659ea6d0a2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05da47a81a79467b91fcd8db4f3476fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172ba1437550414dae2a411e359e9bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218c8045ba4b45ae8902c9c1b58a3855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction succesfull\n",
      "Number of points extracted:32805\n",
      "Time for processing=50.57953977584839 seconds\n",
      "First 10 rows are printed below\n",
      "Number of points outside the raster extents: 27198\n",
      "The extraction assigns NaN.\n",
      "Number of points in NoData areas within the raster extents: 0\n",
      "The extraction assigns NaN.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c5c8f02bc6d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# use LoDs profiles provided.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_profiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtr_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tr_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msampling_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madd_xy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlod_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlod_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\sandpyper\\sandpyper.py\u001b[0m in \u001b[0;36mextract_profiles\u001b[1;34m(self, mode, tr_ids, sampling_step, lod_mode, add_xy, add_slope, default_nan_values)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlod_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'dsm'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                 \u001b[0mlod_path_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirNameDSM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# run extraction from DSMs and orthos with 1m sampling steps and add X and Y fields to output geodataframe.\n",
    "# use LoDs profiles provided.\n",
    "\n",
    "P.extract_profiles(mode='all',tr_ids='tr_id',sampling_step=1,add_xy=True,lod_mode=lod_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd09663",
   "metadata": {},
   "source": [
    "__Note about missing values (numpy.nan)__\n",
    "\n",
    ">NaNs might come from two different cases:\n",
    ">1. extraction of points generated on transects falling __outside__ of the underlying raster extent\n",
    ">2. points sampled from transect __inside__ the raster extent but containing NoData cells.\n",
    ">\n",
    ">Conveniently, the extraction profile function makes sure that if points fall outside the raster extent (case 1), those elevations are assigned a default nan value, in the NumPy np.nan form.\n",
    ">In case 2, however, the values extracted depends on the definition of NaNs of the source raster format. In case the rasters provided do not have an assigned nan value in their metadata, the paramter __default_nan_values__ can be used to specify one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec78cfe",
   "metadata": {},
   "source": [
    "Done!\n",
    "\n",
    "Now the profile attribute of the ProfileSet object we are working with (here, *P*) stores the geopandas.GeoDataFrame containing the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax= plt.subplots(figsize=(10,8), squeeze=True)\n",
    "ortho_path=r\"C:\\my_packages\\sandpyper\\tests\\test_data\\orthos_1m\\leo_20180606_ortho_resampled_1m.tif\"\n",
    "\n",
    "with ras.open(ortho_path,'r') as ortho:\n",
    "\n",
    "    show(ortho, ax=ax)\n",
    "\n",
    "\n",
    "P.profiles.query(\"location=='leo' and raw_date==20180606\").plot(ax=ax,column='z', cmap='RdBu_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de7ea2",
   "metadata": {},
   "source": [
    "As you can see, the profiles include points extracted over areas that are not beach sediment, such as water, swash (inaccurate modeled elevation), vegetation or anything else that can be found aôn or around a beach that is not sand.\n",
    "\n",
    "Computing mutitemporal elevation changes from a dataset of this kind would lead in very erroneous computation as it would include all the aforementioned non-sand points. Therefore, we need to clip out the water from each dataset, retain only a small area of interest landward that the principal foredune (if any) and classify each extracted point as sand or no-sand.\n",
    "\n",
    "Let's see how we can achieve this using sandpyper and Qgis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07029ae3",
   "metadata": {},
   "source": [
    "## Unsupervised clustering and labelling\n",
    "( for method information, please check the documentation, [here](https://npucino.github.io/sandpyper/) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da4fac",
   "metadata": {},
   "source": [
    "### The challenge\n",
    "In order to help discriminate and deal with non-sand points in our dataset, we can use Machine Learning (ML) to classify which points are sand and which ones are not.\n",
    "\n",
    "Currently, sandpyper only facilitates the use of KMeans unsupervised clustering algorithm to accomplish this task. KMeans assigns points sharing similarities in the attribute space to one of a predefined number of clusters and assign these clusters one label. After the clustering is done, we, the human operator, need to visually look at the clustering results in a GIS, overlay the points on the orthophoto, visualise their color-coded labels and 'attach a meaning' to those clusters, which can be quite difficult as ML sees quite differently from us.\n",
    "\n",
    "Of course one could also be tempted to use supervised machine learning techniques, however, considering that the cameras mounted on UAVs are not calibrated and the RGB images are not corrected for illumination variations, the training process would require a very big training dataset which must capture as much as variance in the classes of interest as possible.\n",
    "In this case, the unsupervised procedure facilitated by sandpyper could help in constructing an high quality training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b78f4",
   "metadata": {},
   "source": [
    "### Iterative Silhouette analysis with inflexion search\n",
    "One of the most important parameter we must set before running KMeans is how many clusters we want to partition our points in. Considering that the next step is our visual checking of each cluster, we should choose a number that is not too high, as it would require too much time to visually assign a class to each cluster, but neighter too low, as this would dramatically increase the likelyhood of finding mixed clusters containing both points that are sand and no-sand.\n",
    "\n",
    "In order to let the data guide our choice of number of clusters to use, sandpyper implements an automated iterative Silhouette analysis with inflexion search algorithm.\n",
    "This algorithms use Silhouette coefficients to compute the clustering performances and allows to return a sub-optimal number of clusters to use in each survey with KMeans which minimise the likelyhood of having mixed clusters while also retaining a low number of clusters.\n",
    "\n",
    "Sandpyper implements this in 3 steps:\n",
    "\n",
    "1. __get_sil_location__: iteratively perform KMeans clustering and Silhouette Analysis with increasing number of clusters (k, specified in the `ks` parameter) for every survey, using the feature set specified in the parameter `feature_set`\n",
    "2. __get_opt_k__ : uses the dataframe with Average Silhouette scores with different k for all surveys created by __get_sil_location__ function to find a sub-optimal number of clusters for each survey\n",
    "3. __kmeans_sa__ method: to run KMeans algorithm with the specified sub-optimal number of clusters (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e2993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run interatively KMeans + SA using the feature_set provided\n",
    "#feel free to add \n",
    "\n",
    "feature_set=[\"band1\",\"band2\",\"band3\",\"distance\"]\n",
    "sil_df=get_sil_location(P.profiles,\n",
    "                        ks=(2,15), \n",
    "                        feature_set=feature_set,\n",
    "                       random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aad931",
   "metadata": {},
   "source": [
    "Find sub-optimal k by searching inflexion points where an additional cluster do not considerably degrade the overall clustering performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_k=get_opt_k(sil_df, sigma=0 )\n",
    "opt_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851a6df",
   "metadata": {},
   "source": [
    "If we are not satisfied with the sub-optimal k returned by the algorithm, we can manually specify each survey k\n",
    "by defining a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on our observations on a dataset comprising 87 surveys, 10 clusters (k=10) is generally a good tradeoff.\n",
    "\n",
    "opt_k={'leo_20180606': 10,\n",
    " 'leo_20180713': 10,\n",
    " 'leo_20180920': 10,\n",
    " 'leo_20190211': 10,\n",
    " 'leo_20190328': 10,\n",
    " 'leo_20190731': 10,\n",
    " 'mar_20180601': 10,\n",
    " 'mar_20180621': 10,\n",
    " 'mar_20180727': 10,\n",
    " 'mar_20180925': 10,\n",
    " 'mar_20181113': 10,\n",
    " 'mar_20181211': 10,\n",
    " 'mar_20190205': 10,\n",
    " 'mar_20190313': 10,\n",
    " 'mar_20190516': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb1e68",
   "metadata": {},
   "source": [
    "or, update one value only. For instance, in mar_2019-05-16 dataset, it is unlikely that 3 clusters are enough.<br>\n",
    "So, we replace only that value with 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt_k['mar_2019-05-16']=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdfb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set=[\"band1\",\"band2\",\"band3\",\"distance\"]\n",
    "\n",
    "P.kmeans_sa(opt_k,feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469cc1e",
   "metadata": {},
   "source": [
    "DONE! Now, the profile dataframe attribute has an additional column named __label_k__, which is the result of the survey specific KMeans algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3399474",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.profiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d77d6d",
   "metadata": {},
   "source": [
    "Now that we have labelled the data points, we need to:\n",
    "1. export the dataframe as csv\n",
    "2. import it into your favourite GIS and use the __coordinates__ column as geometry\n",
    "3. using the filtering options, visualise only one survey at a time and assign one color per unique label_k value\n",
    "4. overlay these points on the orthophoto of the survey they were extracted from\n",
    "5. take notes of which label_k number is what class (sand, no_sand at least)\n",
    "6. create the label corrections file (polygons), following the specifications descirbed in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3825193",
   "metadata": {},
   "source": [
    "## Sand labeling and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab27e4",
   "metadata": {},
   "source": [
    "Here below I provided the classes dictionaries of the demo data clustering results.\n",
    "\n",
    "Each class has its own dictionary. The keys store the survey identifier tag ('LocCode_yyyymmdd') and the values are lists of label_k associated with the dictionary class.\n",
    "\n",
    "For instance in this example:\n",
    "\n",
    "```python\n",
    "sand_dict = {'leo_20180606':[5],\n",
    "            'leo_20180713':[1,3,4],\n",
    "            'mar_20180920':[]}\n",
    "            \n",
    "water_dict = {'leo_20180606':[4],\n",
    "            'leo_20180713':[2,6],\n",
    "            'mar_20180920':[1,2,3,4]}\n",
    "```\n",
    "\n",
    "In the St. Leonards survey of the 13th July 2018, the label_k 1,3 and 4 are sand, while the label_k 2 and 6 are water.\n",
    "In Marengo, the 20th September 2018, no label_k represents sand while 1,2,3 and 4 are water.\n",
    "\n",
    "Here below are reported the label dictionaries of the demo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b757b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_dict={'leo_20180606':[0,9,10],\n",
    "'leo_20180713':[0,3,4,7],\n",
    "'leo_20180920':[0,2,6,7],\n",
    "'leo_20190211':[0,2,5],\n",
    "'leo_20190328':[2,4,5],\n",
    "'leo_20190731':[0,2,8,6],\n",
    "'mar_20180601':[1,6],\n",
    "'mar_20180621':[4,6],\n",
    "'mar_20180727':[0,5,9,10],\n",
    "'mar_20180925':[6],\n",
    "'mar_20181113':[1],\n",
    "'mar_20181211':[4],\n",
    "'mar_20190205':[],\n",
    "'mar_20190313':[],\n",
    "'mar_20190516':[4,7]}\n",
    "\n",
    "no_sand_dict={'leo_20180606':[5],\n",
    "'leo_20180713':[],\n",
    "'leo_20180920':[],\n",
    "'leo_20190211':[1],\n",
    "'leo_20190328':[],\n",
    "'leo_20190731':[1],\n",
    "'mar_20180601':[4,5],\n",
    "'mar_20180621':[3,5],\n",
    "'mar_20180727':[4,7],\n",
    "'mar_20180925':[5],\n",
    "'mar_20181113':[0],\n",
    "'mar_20181211':[0],\n",
    "'mar_20190205':[0,5],\n",
    "'mar_20190313':[4],\n",
    "'mar_20190516':[2,5]}\n",
    "\n",
    "veg_dict={'leo_20180606':[1,3,7,8],\n",
    "'leo_20180713':[1,5,9],\n",
    "'leo_20180920':[1,4,5],\n",
    "'leo_20190211':[4],\n",
    "'leo_20190328':[0,1,6],\n",
    "'leo_20190731':[3,7],\n",
    "'mar_20180601':[0,7],\n",
    "'mar_20180621':[1,7],\n",
    "'mar_20180727':[1,3],\n",
    "'mar_20180925':[1,3],\n",
    "'mar_20181113':[3],\n",
    "'mar_20181211':[2],\n",
    "'mar_20190205':[3],\n",
    "'mar_20190313':[1,5],\n",
    "'mar_20190516':[0]}\n",
    "\n",
    "sand_dict={'leo_20180606':[2,4,6],\n",
    "'leo_20180713':[2,6,8],\n",
    "'leo_20180920':[3],\n",
    "'leo_20190211':[3],\n",
    "'leo_20190328':[3],\n",
    "'leo_20190731':[4,5],\n",
    "'mar_20180601':[2,3],\n",
    "'mar_20180621':[0,2],\n",
    "'mar_20180727':[2,6,8],\n",
    "'mar_20180925':[0,4,2],\n",
    "'mar_20181113':[2,4],\n",
    "'mar_20181211':[3,1],\n",
    "'mar_20190205':[1,2,4],\n",
    "'mar_20190313':[0,2,3],\n",
    "'mar_20190516':[1,3,6]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5418be7",
   "metadata": {},
   "source": [
    "Now that we have each class dictionary, let's put all of those in another dictionary in order to specify the name of the class.\n",
    "Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faac7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_dicts={'no_sand': no_sand_dict,\n",
    "         'sand': sand_dict,\n",
    "        'water': water_dict,\n",
    "        'veg':veg_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b0935",
   "metadata": {},
   "source": [
    "Now, let's provide the paths to the geopackages (preferred, but also shapefiles are accepted) containing the label corrections, the watermasks and the shoremasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e509dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_corrections_path=r\"C:\\my_packages\\sandpyper\\tests\\test_data\\clean\\label_corrections.gpkg\"\n",
    "watermasks_path=r\"C:\\my_packages\\sandpyper\\tests\\test_data\\clean\\watermasks.gpkg\"\n",
    "shoremasks_path=r\"C:\\my_packages\\sandpyper\\tests\\test_data\\clean\\shoremasks.gpkg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f004b",
   "metadata": {},
   "source": [
    "All set!\n",
    "\n",
    "Now we are ready to use the __.cleanit method__ to:\n",
    "\n",
    "1. classify each point using the dictionaries provided\n",
    "2. fine-tuning the label_k using the label correction polygons provided\n",
    "3. eliminate water/swash points using the watermasks provided\n",
    "4. clip to the shore area using the shoremasks provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.cleanit(l_dicts=l_dicts,\n",
    "          watermasks_path=watermasks_path,\n",
    "          shoremasks_path=shoremasks_path,\n",
    "          label_corrections_path=label_corrections_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006080d",
   "metadata": {},
   "source": [
    "DONE!\n",
    "\n",
    "Now we have a cleaned and tidy classified dataset of elevation profiles, ready to be analysed to extract sediment specific altimetric, volumetric and behavioural dynamics information.\n",
    "\n",
    "A new attribute has been stored, *ProfileSet.cleaning_steps*, which is a list of the cleaning steps applied to the current profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.cleaning_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39ae1ac",
   "metadata": {},
   "source": [
    "## Save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2643bbb3",
   "metadata": {},
   "source": [
    "To save the objects, just call the method *ProfileSet.save* with only two arguments:\n",
    "1. the path to the folder where to save the data.\n",
    "2. the name of the file you want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_out=r'C:\\my_packages\\sandpyper\\tests\\test_data'\n",
    "\n",
    "name=\"test\"\n",
    "P.save(name,dir_out)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ced0b5",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72446987",
   "metadata": {},
   "source": [
    "In this notebook we extracted the elevation and colour information from all our rasters and used it with an iterative Silhouette analysis and KMeans algorithm to unsupervisingly partition the data into clusters of similar points. Then, we assigned meaning of these labels using an external GIS and cleaned and clipped the data into our final slassified dataset. In the next notebook we will finally obtain multi-scale information about sediment dynamics in each site/transect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311186ab",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
