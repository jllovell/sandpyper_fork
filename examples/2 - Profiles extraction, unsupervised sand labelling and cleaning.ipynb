{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c902d8b-1a3e-4eb8-887f-b6a2f230fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JLL edit: add cell to check we are in the right jupyter kernel for our virtual environment\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8c18a",
   "metadata": {},
   "source": [
    "<font size=\"5\"><center> <b>Sandpyper: sandy beaches SfM-UAV analysis tools</b></center></font>\n",
    "\n",
    "![im](images/banner.png)\n",
    "\n",
    "\n",
    "<font face=\"Calibri\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f277f",
   "metadata": {},
   "source": [
    "# Profile extraction, LoD and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee748d",
   "metadata": {},
   "source": [
    "<font size=\"3\"> <b> Nicolas Pucino; PhD Student @ Deakin University, Australia </b> <br>\n",
    "\n",
    "<b>This notebook covers the following concepts:</b>\n",
    "\n",
    "- The ProfileSet class.\n",
    "- Data extraction.\n",
    "- Limit of Detections (LoDs).\n",
    "</font>\n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import rasterio as ras\n",
    "from rasterio.plot import show\n",
    "\n",
    "from sandpyper.sandpyper import ProfileSet\n",
    "from sandpyper.common import get_sil_location, get_opt_k\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf4119",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ee8a3",
   "metadata": {},
   "source": [
    "The __ProfileSet__ class is an object that contains all the information necessary to carry a typical *sandpyper* analysis from start to end. Once instantiated, it will contains a number of global variables that store some fundamental monitoring charasteristics such as:\n",
    "\n",
    "* paths to the data directories\n",
    "* coordinate reference systems of each site\n",
    "* location codes and search keywords\n",
    "* some monitoring parameters such as alongshore transect spacing, acrosshore sampling step and cleaning steps used\n",
    "* and of course, the elevation and colour profile data.\n",
    "\n",
    "Moreover, a few key methods will:\n",
    "\n",
    "1. extract data from the provided transects\n",
    "2. cluster extracted points using unsupervised clustering algorithm KMeans\n",
    "3. clean the dataset with user-provided watermasks, shoremasks and fine-tuning polygons.\n",
    "\n",
    "Moreover, when the __ProfileSet__ class is instantiated, it prints out the number of DSMs and orthophotos found for each location in the provided folders and it also creates a dataframe storing filenames, CRSs, location codes and raw dates extracted from the filenames. This dataframe is useful as a sanity check to see if all the information are correct before proceeding with the actual profile extraction. It will be stored as the attribute *ProfileSet.check*.\n",
    "\n",
    "Let's see here how it all works...\n",
    "\n",
    "First, create the required monitoring global settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61345684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLL edit: changed to my path to the folder\n",
    "\n",
    "# set the path to the test data folder\n",
    "test_data_folder = r\"/Users/joshualeonlovell/Desktop/Work/LUH_HiWi_LuFi_JT/coauthor/sandpyper_fork/examples/test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLL edits: changed to mac compatible paths, added spaces for readability\n",
    "\n",
    "# the paths to the DSM, orthophotos and transect directories\n",
    "dirNameDSM   = Path(test_data_folder + r\"/dsm_1m\")\n",
    "dirNameOrtho = Path(test_data_folder + r\"/orthos_1m\")\n",
    "dirNameTrans = Path(test_data_folder + r\"/transects\")\n",
    "\n",
    "# path to the LoD transects\n",
    "\n",
    "lod_mode = Path(test_data_folder + r\"/lod_transects\")\n",
    "\n",
    "\n",
    "# the location codes used for the monitored locations\n",
    "loc_codes = [\"mar\",\"leo\"]\n",
    "\n",
    "\n",
    "# the keyword search dictionary\n",
    "loc_search_dict = {   'leo': ['St','Leonards','leonards','leo'],\n",
    "                   'mar': ['Marengo','marengo','mar'] }\n",
    "\n",
    "\n",
    "# the EPSG codes of the coordinate reference systems for each location code (location) given in CRS string format\n",
    "crs_dict_string = {\n",
    "                  'mar': {'init': 'epsg:32754'},\n",
    "                  'leo':{'init': 'epsg:32755'}\n",
    "                   }\n",
    "\n",
    "# the transect spacing of the transects\n",
    "transects_spacing = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707dea5f",
   "metadata": {},
   "source": [
    "Then, create an instance of the ProfileSet class. Let's set 'check' parameter to 'all' to create the check dataframe\n",
    "and store it in the variable P.check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39067452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLL edit: added spaces for readability\n",
    "\n",
    "P = ProfileSet(dirNameDSM=dirNameDSM,\n",
    "               dirNameOrtho=dirNameOrtho,\n",
    "               dirNameTrans=dirNameTrans,\n",
    "               transects_spacing=transects_spacing,\n",
    "               loc_codes=loc_codes,\n",
    "               loc_search_dict=loc_search_dict,\n",
    "               crs_dict_string=crs_dict_string,\n",
    "               check=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e738d0",
   "metadata": {},
   "source": [
    "Check the infromation extracted from the formatted filenames.\n",
    "All the CRSs in a line (survey) must match!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d66e350",
   "metadata": {},
   "source": [
    "## Extraction of profiles from folder\n",
    "\n",
    "After checking the check dataframe and all looks right, we are ready to actually extract the data from both DSMs and orhtophotos using the *.extract_profiles* method.\n",
    "Importantly, we also extract the data for the Limit of Detections (LoD) computation, using some transects digitised over areas we expect __NOT TO BE CHANGING__ in elevation during the monitoring period, also known as calibration zones.\n",
    "\n",
    "Thus, the most important parameters to set are:\n",
    "\n",
    "1. __sampling step__: indicates the __cross-shore sampling distance (m)__ that we want to use along our transects. Beware, although you could use a very small sampling distance (UAV datasets tend to be between few to 10 cm pixel size), file dimension will increase significantly!.\n",
    "\n",
    "2. __lod__: this can be the path to a directory containing the lod transects to use as lod, a number to use as lod for all surveys in all locations or None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05de363",
   "metadata": {},
   "source": [
    "The parameter __mode__ is here set to 'all' as we want to extract pixel values from both the DSMs and orthophotos, while the parameter __tr_ids__ specifies the name of the field in the transect file is used to store the transect IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032dde91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLL edit: added spaces for readability\n",
    "\n",
    "# run extraction from DSMs and orthos with 1m sampling steps and add X and Y fields to output geodataframe.\n",
    "# use LoDs profiles provided.\n",
    "\n",
    "P.extract_profiles(mode='all', tr_ids='tr_id', sampling_step=1, add_xy=True, lod_mode=lod_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd09663",
   "metadata": {},
   "source": [
    "__Note about missing values (numpy.nan)__\n",
    "\n",
    ">NaNs might come from two different cases:\n",
    ">1. extraction of points generated on transects falling __outside__ of the underlying raster extent\n",
    ">2. points sampled from transect __inside__ the raster extent but containing NoData cells.\n",
    ">\n",
    ">Conveniently, the extraction profile function makes sure that if points fall outside the raster extent (case 1), those elevations are assigned a default nan value, in the NumPy np.nan form.\n",
    ">In case 2, however, the values extracted depends on the definition of NaNs of the source raster format. In case the rasters provided do not have an assigned nan value in their metadata, the paramter __default_nan_values__ can be used to specify one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec78cfe",
   "metadata": {},
   "source": [
    "Done!\n",
    "\n",
    "Now the profile attribute of the ProfileSet object we are working with (here, *P*) stores the geopandas.GeoDataFrame containing the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLL edit: added and removed spaces for readability, adapted path for Mac (forward slashes), added %matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10,8), squeeze=True)\n",
    "ortho_path = Path(test_data_folder + r\"/orthos_1m/leo_20180606_ortho_resampled_1m.tif\")\n",
    "\n",
    "with ras.open(ortho_path,'r') as ortho:\n",
    "    show(ortho, ax=ax)\n",
    "\n",
    "P.profiles.query(\"location=='leo' and raw_date==20180606\").plot(ax=ax, column='z', cmap='RdBu_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de7ea2",
   "metadata": {},
   "source": [
    "As you can see, the profiles include points extracted over areas that are not beach sediment, such as water, swash (inaccurate modeled elevation), vegetation or anything else that can be found aôn or around a beach that is not sand.\n",
    "\n",
    "Computing mutitemporal elevation changes from a dataset of this kind would lead in very erroneous computation as it would include all the aforementioned non-sand points. Therefore, we need to clip out the water from each dataset, retain only a small area of interest landward that the principal foredune (if any) and classify each extracted point as sand or no-sand.\n",
    "\n",
    "Let's see how we can achieve this using sandpyper and Qgis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07029ae3",
   "metadata": {},
   "source": [
    "## Unsupervised clustering and labelling\n",
    "( for method information, please check the documentation, [here](https://npucino.github.io/sandpyper/) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da4fac",
   "metadata": {},
   "source": [
    "### The challenge\n",
    "In order to help discriminate and deal with non-sand points in our dataset, we can use Machine Learning (ML) to classify which points are sand and which ones are not.\n",
    "\n",
    "Currently, sandpyper only facilitates the use of KMeans unsupervised clustering algorithm to accomplish this task. KMeans assigns points sharing similarities in the attribute space to one of a predefined number of clusters and assign these clusters one label. After the clustering is done, we, the human operator, need to visually look at the clustering results in a GIS, overlay the points on the orthophoto, visualise their color-coded labels and 'attach a meaning' to those clusters, which can be quite difficult as ML sees quite differently from us.\n",
    "\n",
    "Of course one could also be tempted to use supervised machine learning techniques, however, considering that the cameras mounted on UAVs are not calibrated and the RGB images are not corrected for illumination variations, the training process would require a very big training dataset which must capture as much as variance in the classes of interest as possible.\n",
    "In this case, the unsupervised procedure facilitated by sandpyper could help in constructing an high quality training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b78f4",
   "metadata": {},
   "source": [
    "### Iterative Silhouette analysis with inflexion search\n",
    "One of the most important parameter we must set before running KMeans is how many clusters we want to partition our points in. Considering that the next step is our visual checking of each cluster, we should choose a number that is not too high, as it would require too much time to visually assign a class to each cluster, but neighter too low, as this would dramatically increase the likelyhood of finding mixed clusters containing both points that are sand and no-sand.\n",
    "\n",
    "In order to let the data guide our choice of number of clusters to use, sandpyper implements an automated iterative Silhouette analysis with inflexion search algorithm.\n",
    "This algorithms use Silhouette coefficients to compute the clustering performances and allows to return a sub-optimal number of clusters to use in each survey with KMeans which minimise the likelyhood of having mixed clusters while also retaining a low number of clusters.\n",
    "\n",
    "Sandpyper implements this in 3 steps:\n",
    "\n",
    "1. __get_sil_location__: iteratively perform KMeans clustering and Silhouette Analysis with increasing number of clusters (k, specified in the `ks` parameter) for every survey, using the feature set specified in the parameter `feature_set`\n",
    "2. __get_opt_k__ : uses the dataframe with Average Silhouette scores with different k for all surveys created by __get_sil_location__ function to find a sub-optimal number of clusters for each survey\n",
    "3. __kmeans_sa__ method: to run KMeans algorithm with the specified sub-optimal number of clusters (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e2993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JLL edits: added spaces for readability, corrected grammar in comments, added question to comment I don't understand\n",
    "\n",
    "# Run iteratively KMeans + SA using the feature_set provided\n",
    "#feel free to add (JLL: add what?)\n",
    "\n",
    "feature_set = [\"band1\",\"band2\",\"band3\",\"distance\"]\n",
    "sil_df = get_sil_location(P.profiles,\n",
    "                          ks=(2,15), \n",
    "                          feature_set=feature_set,\n",
    "                          random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aad931",
   "metadata": {},
   "source": [
    "Find sub-optimal k by searching inflexion points where an additional cluster do not considerably degrade the overall clustering performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLL edit: added and removed spaces for readability\n",
    "\n",
    "opt_k = get_opt_k(sil_df, sigma=0)\n",
    "opt_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851a6df",
   "metadata": {},
   "source": [
    "If we are not satisfied with the sub-optimal k returned by the algorithm, we can manually specify each survey k\n",
    "by defining a dictionary.\n",
    "\n",
    "Based on our observations on a dataset comprising 87 surveys, __10 clusters (k=10)__ is generally a good tradeoff, and set \n",
    "\n",
    "```python\n",
    "opt_k={'leo_20180606': 10,\n",
    " 'leo_20180713': 10,\n",
    " 'leo_20180920': 10,\n",
    " 'leo_20190211': 10,\n",
    " 'leo_20190328': 10,\n",
    " 'leo_20190731': 10,\n",
    " 'mar_20180601': 10,\n",
    " 'mar_20180621': 10,\n",
    " 'mar_20180727': 10,\n",
    " 'mar_20180925': 10,\n",
    " 'mar_20181113': 10,\n",
    " 'mar_20181211': 10,\n",
    " 'mar_20190205': 10,\n",
    " 'mar_20190313': 10,\n",
    " 'mar_20190516': 10}\n",
    "```\n",
    "or, we could update one value only. For instance, we could change __mar_2019-05-16 dataset__ to 10 clusters simply by updating the existing `òpt_k` dictionary:\n",
    "```python\n",
    "opt_k={'mar_20190516': 10}\n",
    "```\n",
    "\n",
    "However, for the purpose of this test workflow, __please keep the original opt_k dictionary that Sandpyper computed__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdfb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLL edit: added spaces for readability\n",
    "\n",
    "feature_set = [\"band1\",\"band2\",\"band3\",\"distance\"]\n",
    "\n",
    "P.kmeans_sa(opt_k, feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469cc1e",
   "metadata": {},
   "source": [
    "DONE! Now, the profile dataframe attribute has an additional column named __label_k__, which is the result of the survey specific KMeans algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9ddd0",
   "metadata": {},
   "source": [
    "We can now export the profile dataset as CSV in order to the open it as a comma-delimited file in your favourite GIS.\n",
    "Use this command to save it to your favourite path:\n",
    "\n",
    "```python\n",
    "P.profiles.to_csv(\"your/favourite/location/profiles.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d77d6d",
   "metadata": {},
   "source": [
    "Now that we have labelled the data points and exported as a CSV file, we need to:\n",
    "\n",
    "1. import it into your favourite GIS and use the __coordinates__ column as geometry\n",
    "2. using the filtering options, visualise only one survey at a time and assign one color per unique label_k value\n",
    "3. overlay these points on the orthophoto of the survey they were extracted from\n",
    "4. take notes of which label_k number is what class (sand, no_sand at least)\n",
    "5. create the label corrections file (polygons), following the specifications descirbed in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3825193",
   "metadata": {},
   "source": [
    "## Sand labeling and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab27e4",
   "metadata": {},
   "source": [
    "Here below I provided the classes dictionaries of the test data clustering results.\n",
    "\n",
    "Each class has its own dictionary. The keys store the survey identifier tag ('LocCode_yyyymmdd') and the values are lists of label_k associated with the dictionary class.\n",
    "\n",
    "For instance in this example:\n",
    "\n",
    "```python\n",
    "sand_dict = {'leo_20180606':[5],\n",
    "            'leo_20180713':[1,3,4],\n",
    "            'mar_20180920':[]}\n",
    "            \n",
    "water_dict = {'leo_20180606':[4],\n",
    "            'leo_20180713':[2,6],\n",
    "            'mar_20180920':[1,2,3,4]}\n",
    "```\n",
    "\n",
    "In the St. Leonards survey of the 13th July 2018, the label_k 1,3 and 4 are sand, while the label_k 2 and 6 are water.\n",
    "In Marengo, the 20th September 2018, no label_k represents sand while 1,2,3 and 4 are water.\n",
    "\n",
    "Here below are reported the label dictionaries of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b757b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLL edit: added spaces for readability\n",
    "\n",
    "water_dict = {'leo_20180606':[0,9,10],\n",
    "              'leo_20180713':[0,3,4,7],\n",
    "              'leo_20180920':[0,2,6,7],\n",
    "              'leo_20190211':[0,2,5],\n",
    "              'leo_20190328':[2,4,5],\n",
    "              'leo_20190731':[0,2,8,6],\n",
    "              'mar_20180601':[1,6],\n",
    "              'mar_20180621':[4,6],\n",
    "              'mar_20180727':[0,5,9,10],\n",
    "              'mar_20180925':[6],\n",
    "              'mar_20181113':[1],\n",
    "              'mar_20181211':[4],\n",
    "              'mar_20190205':[],\n",
    "              'mar_20190313':[],\n",
    "              'mar_20190516':[4,7]}\n",
    "\n",
    "no_sand_dict = {'leo_20180606':[5],\n",
    "                'leo_20180713':[],\n",
    "                'leo_20180920':[],\n",
    "                'leo_20190211':[1],\n",
    "                'leo_20190328':[],\n",
    "                'leo_20190731':[1],\n",
    "                'mar_20180601':[4,5],\n",
    "                'mar_20180621':[3,5],\n",
    "                'mar_20180727':[4,7],\n",
    "                'mar_20180925':[5],\n",
    "                'mar_20181113':[0],\n",
    "                'mar_20181211':[0],\n",
    "                'mar_20190205':[0,5],\n",
    "                'mar_20190313':[4],\n",
    "                'mar_20190516':[2,5]}\n",
    "\n",
    "veg_dict = {'leo_20180606':[1,3,7,8],\n",
    "            'leo_20180713':[1,5,9],\n",
    "            'leo_20180920':[1,4,5],\n",
    "            'leo_20190211':[4],\n",
    "            'leo_20190328':[0,1,6],\n",
    "            'leo_20190731':[3,7],\n",
    "            'mar_20180601':[0,7],\n",
    "            'mar_20180621':[1,7],\n",
    "            'mar_20180727':[1,3],\n",
    "            'mar_20180925':[1,3],\n",
    "            'mar_20181113':[3],\n",
    "            'mar_20181211':[2],\n",
    "            'mar_20190205':[3],\n",
    "            'mar_20190313':[1,5],\n",
    "            'mar_20190516':[0]}\n",
    "\n",
    "sand_dict = {'leo_20180606':[2,4,6],\n",
    "             'leo_20180713':[2,6,8],\n",
    "             'leo_20180920':[3],\n",
    "             'leo_20190211':[3],\n",
    "             'leo_20190328':[3],\n",
    "             'leo_20190731':[4,5],\n",
    "             'mar_20180601':[2,3],\n",
    "             'mar_20180621':[0,2],\n",
    "             'mar_20180727':[2,6,8],\n",
    "             'mar_20180925':[0,4,2],\n",
    "             'mar_20181113':[2,4],\n",
    "             'mar_20181211':[3,1],\n",
    "             'mar_20190205':[1,2,4],\n",
    "             'mar_20190313':[0,2,3],\n",
    "             'mar_20190516':[1,3,6]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5418be7",
   "metadata": {},
   "source": [
    "Now that we have each class dictionary, let's put all of those in another dictionary in order to specify the name of the class.\n",
    "Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faac7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLL edit: added spaces for readability\n",
    "\n",
    "l_dicts = {'no_sand': no_sand_dict,\n",
    "           'sand': sand_dict,\n",
    "           'water': water_dict,\n",
    "           'veg':veg_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b0935",
   "metadata": {},
   "source": [
    "Now, let's provide the paths to the geopackages (preferred, but also shapefiles are accepted) containing the label corrections, the watermasks and the shoremasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e509dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLL edit: added spaces for readability, made paths mac compatible\n",
    "\n",
    "label_corrections_path = Path(test_data_folder + r\"/clean/label_corrections.gpkg\")\n",
    "watermasks_path        = Path(test_data_folder + r\"/clean/watermasks.gpkg\")\n",
    "shoremasks_path        = Path(test_data_folder + r\"/clean/shoremasks.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f004b",
   "metadata": {},
   "source": [
    "All set!\n",
    "\n",
    "Now we are ready to use the __.cleanit method__ to:\n",
    "\n",
    "1. classify each point using the dictionaries provided\n",
    "2. fine-tuning the label_k using the label correction polygons provided\n",
    "3. eliminate water/swash points using the watermasks provided\n",
    "4. clip to the shore area using the shoremasks provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.cleanit(l_dicts=l_dicts,\n",
    "          watermasks_path=watermasks_path,\n",
    "          shoremasks_path=shoremasks_path,\n",
    "          label_corrections_path=label_corrections_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006080d",
   "metadata": {},
   "source": [
    "DONE!\n",
    "\n",
    "Now we have a cleaned and tidy classified dataset of elevation profiles, ready to be analysed to extract sediment specific altimetric, volumetric and behavioural dynamics information.\n",
    "\n",
    "A new attribute has been stored, *ProfileSet.cleaning_steps*, which is a list of the cleaning steps applied to the current profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.cleaning_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39ae1ac",
   "metadata": {},
   "source": [
    "## Save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2643bbb3",
   "metadata": {},
   "source": [
    "To save the objects, just call the method *ProfileSet.save* with only two arguments:\n",
    "1. the path to the folder where to save the data.\n",
    "2. the name of the file you want to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLL edit: changed the path to the test data folder, added and removed spaces for readability\n",
    "\n",
    "dir_out = test_data_folder\n",
    "name    = \"test.p\"\n",
    "P.save(name, dir_out)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ced0b5",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72446987",
   "metadata": {},
   "source": [
    "In this notebook we extracted the elevation and colour information from all our rasters and used it with an iterative Silhouette analysis and KMeans algorithm to unsupervisingly partition the data into clusters of similar points. Then, we assigned meaning of these labels using an external GIS and cleaned and clipped the data into our final slassified dataset. In the next notebook we will finally obtain multi-scale information about sediment dynamics in each site/transect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ed84a",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
