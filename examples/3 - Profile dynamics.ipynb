{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca8c18a",
   "metadata": {},
   "source": [
    "<font size=\"5\"><center> <b>Sandpyper: sandy beaches SfM-UAV analysis tools</b></center></font>\n",
    "\n",
    "    \n",
    "<center><img src=\"images/banner.png\" width=\"80%\"  /></center>\n",
    "\n",
    "<font face=\"Calibri\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2021bff",
   "metadata": {},
   "source": [
    "# Multiscale sediment dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c0c79",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=\"3\"> <b> Nicolas Pucino; PhD Student @ Deakin University, Australia </b> <br>\n",
    "\n",
    "<b>This notebook covers the following concepts:</b>\n",
    "\n",
    "- The ProfileDynamics class\n",
    "- Multiscale volumetric analysis and plotting\n",
    "- Hotspot analysis\n",
    "- Multiscale beachface Cluster Dynamics indices and plotting\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1bcf483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\pysal\\explore\\segregation\\network\\network.py:15: UserWarning: You need pandana and urbanaccess to work with segregation's network module\n",
      "You can install them with  `pip install urbanaccess pandana` or `conda install -c udst pandana urbanaccess`\n",
      "  warn(\n",
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\pysal\\model\\spvcm\\abstracts.py:10: UserWarning: The `dill` module is required to use the sqlite backend fully.\n",
      "  from .sqlite import head_to_sql, start_sql\n",
      "C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "from sandpyper.hotspot import ProfileDynamics\n",
    "from sandpyper.dynamics import sensitivity_tr_rbcd, plot_sensitivity_rbcds_transects\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83df4a72",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (profile.py, line 70)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3441\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-3903ed910ac7>\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    P=pickle.load(open(r\"C:\\my_packages\\sandpyper\\tests\\test_data\\test.p\", \"rb\"))\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\conda3\\envs\\sandpyper_env\\lib\\site-packages\\sandpyper\\profile.py\"\u001b[1;36m, line \u001b[1;32m70\u001b[0m\n\u001b[1;33m    self.=loc_search_dict\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "P=pickle.load(open(r\"C:\\my_packages\\sandpyper\\tests\\test_data\\test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc769f5e",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ace853",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac1d79",
   "metadata": {},
   "source": [
    "In this notebook, we finally get some dynamics data out of our profiles using the __ProfileDynamics__ class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a708f14",
   "metadata": {},
   "source": [
    "### ProfileDynamics class\n",
    "\n",
    "The __ProfileDynamics__ class is the core object in sandpyper. Its methods allows to:\n",
    "\n",
    "* compute multitemporal elevation changes\n",
    "* compute Limit of Detections (LoDs) statistics\n",
    "* compute multi-scale volumetrics\n",
    "* discretise elevation changes into discrete classes of magnite of changes with a variety of methods (thanks to the Python package [pysal](https://pysal.org/))\n",
    "* compute multi-scale beachface cluster dynamics indices (BCDs, check the paper [Pucino et al., 2021](https://www.nature.com/articles/s41598-021-83477-6.epdf?sharing_token=844tVeK37MSYJoNEThYGItRgN0jAjWel9jnR3ZoTv0PxEZXr8bbQlsWb42PNMTsu53flRyY9FJ3fI5pe-Atcf1RbRfb5zpR7Q0KX8rhHKlN-viewjBsZvEcrYBcliJPElRIl7UYdYDwJS94n1E_4gLskgni0xLdXl8TujAjpfG8%3D))\n",
    "* various plotting capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8559764",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[\"Undefined\", \"Small\", \"Medium\", \"High\", \"Extreme\"]\n",
    "appendix=[\"_deposition\", \"_erosion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f22f84",
   "metadata": {},
   "source": [
    "Here below we instantiate a ProfileDynamics object which discretises the data into 5 bins using the Jenks and Caspall method. We also provides labels to the bins, in order to be able to name them and keep the analysis clear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eea37d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D = ProfileDynamics(P, bins=5, method=\"JenksCaspall\", labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ad552",
   "metadata": {},
   "source": [
    "### Computing multitemporal dataset (dh_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888cf31",
   "metadata": {},
   "source": [
    "The method *ProfileDynamics.compute_multitemporal* allows to:\n",
    "\n",
    "1. create pairwise (date from - date to) elevation difference dataset called dh_df\n",
    "2. option to filter data based on a class. For instance, computing elevationc hanges of sand only\n",
    "3. option to provide a dictionary to use full length location names instead of location codes in some plots (loc_full)\n",
    "\n",
    "Once run, this will add an attribute called *dh_df* that stores the dataframe of elevation changes across all the surveys and time periods.\n",
    "If LoD was previously derived with LoD transects, this method will also compute lod_dh table, which is used to create an LoD table.\n",
    "\n",
    "Let's see how this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.compute_multitemporal(loc_full={'mar': 'Marengo',\n",
    "         'leo': 'St. Leonards'}, filter_class='sand')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c5198",
   "metadata": {},
   "source": [
    "We can see that mutlitemporal statistics have been computed for each time period in all locations, creating the dh_df dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.dh_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86731cf6",
   "metadata": {},
   "source": [
    "The __dt__ column stores a 'time-period ID'. For instance, in St. Leonards, the changes of points from (20180606) to (20180713) is named dt_0, because it is the first time period in this location. The second one, (20180713) to (20180920) is named dt_1 and so on.\n",
    "\n",
    "All the information about time periods are stored into a dataframe as an attribute called *ProfileDynamics.dh_details*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f298ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.dh_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a20a15",
   "metadata": {},
   "source": [
    "In the same way the dh_df dataframes has been computed, the lod_dh table is also computed, except that no filters are applied and all points previously extracted from the LoD transects are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.lod_dh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66090f",
   "metadata": {},
   "source": [
    "From this dataset, a set of useful error (i.e, elevation change in calibration areas where a theoretical zero change should occur) statistics is also computed, which include:\n",
    "\n",
    "1. mean (column: 'mean')\n",
    "2. median (column: 'med')\n",
    "3. standard deviation (column: 'std')\n",
    "4. normalised medain absolute deviation (column: 'nmad')\n",
    "5. 68.3th quantile of absolute error (column: 'a_q683')\n",
    "6. 95th quantile of absolute error (column: 'a_q95')\n",
    "7. robust root mean squared error (column: 'rrmse')\n",
    "8. number of total observations (column: 'n')\n",
    "9. n_outliers using the 3-sigma method (column: 'n_outliers')\n",
    "10. Shapiro–Wilk statistics, p-values and normality evaluation (columns: 'saphiro_stat', 'saphiro_p', 'saphiro_normality')\n",
    "11. D’Agostino–Pearson statistics, p-values and normality evaluation (columns: 'ago_stat', 'ago_p', 'agoo_normality')\n",
    "12. the chosen Limit of Detection based on normality check (column: 'lod')\n",
    "\n",
    "All this information is stored in the **lod_df** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.lod_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34adf64",
   "metadata": {},
   "source": [
    "### Plot: LoD normality check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3cfde8",
   "metadata": {},
   "source": [
    "Eventhough statistical tests can be useful in identifying wether a distribution is normal or not, especially in the case of photogrammetrically derived products, it is advised to also visually check both the histograms and the Q-Q plots of absolute error.\n",
    "\n",
    "Sandpyper allows quick and easy generation of such plots, here is an example for Marengo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8159dc62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "D.plot_lod_normality_check(locations=['leo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b5c4d",
   "metadata": {},
   "source": [
    "## Multiscale volumetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8414198",
   "metadata": {},
   "source": [
    "With the LoD and dh_df tables we are finally ready to observe multi-scale change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a648d",
   "metadata": {},
   "source": [
    "### Mean Elevation Change\n",
    "\n",
    "The __Mean Elevation Change (MEC)__ is calculated with the totality of the valid points (filtered based on class, beyond LoD, within the beachface) that occur both in the pre and post dataset in each location.<br>\n",
    "MEC is the preferred metric as it allows a robust comparison between locations and time, given a context where number of transects and retained valid points can drastically change from time to time or from location to locatio due to changes in:\n",
    "\n",
    "- survey extent\n",
    "- wave run-up and the swash level\n",
    "- non-sand points across the beachface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e6061a",
   "metadata": {},
   "source": [
    "### Volumetric estimation from profiles\n",
    "\n",
    "The __volumes of beachface change__, instead, are estimated, using the following procedure.\n",
    "\n",
    "1. Along each transect, the missing points (due to non-sand filters if applied) are replaced by a linearly interpolated value.\n",
    "2. We integrate the change along the cleaned transect to estimated volumetric change at each transect.\n",
    "3. We multiply the sum of all transect volume change by the transect spacing, to obtain a location-level beachface change estimation.\n",
    "\n",
    "This approximation is necessary to go from transect change to beachface change. Here below is how Sandpyper takes care of that in one line, computing multi-scale MECs and volumetrics in one line, by just calling the *ProfileDynamics.compute_volumetrics* method.\n",
    "\n",
    "This will add two additional dataframes as attributes, named __location_volumetrics__ and __transects_volumetrics__, which will store a number of statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute volumetrics\n",
    "\n",
    "D.compute_volumetrics(lod=D.lod_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc65b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.location_volumetrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49829027",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.transects_volumetrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a3d89",
   "metadata": {},
   "source": [
    "Amongst the varios statistics computed (__check the docs for details__), the most important fields are:\n",
    "\n",
    "- norm_net_change: abs_net_change/total number of points used in the calculation of this time period (n_obs_valid). This is the MEC.\n",
    "- net_vol_change: Net volume change (m3)\n",
    "- location_m3_m: Net volume change / beach length (m3/m)\n",
    "- cum: Cumulative net volume change since the beginning of the monitoring (m3)\n",
    "- cum_mec: Cumulative norm_net_change since the beginning of the monitoring (m3)\n",
    "\n",
    "Let's use sandpyper to visualise these stats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d010fc",
   "metadata": {},
   "source": [
    "### Plot: multi-scale MECS and volumetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ae2f8",
   "metadata": {},
   "source": [
    "#### Transect-level\n",
    "\n",
    "The method *ProfileDynamics.plot_transects* allow to visualise elevation profile changes in selected locations, tr_ids and time periods. COnveniently, if the data has been classified, by setting the parameter __classified=True__ we can also see what class each point was classified into.\n",
    "\n",
    ">__NOTE__: if __classified=False__, the points that have been filtered out will not be displayed. If __classified=True__, then all points will be plotted. This is done to have a visual understanding of the cleaning procedure applied and appreciate what is actually retained to compute change statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot transect ID 10 in Marengo, for dt_2 and dt_3 with no classification applied.\n",
    "\n",
    "D.plot_transects(location='mar', tr_id=10, from_date='20180727', to_date='20180925', classified=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9047f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same, with classification applied.\n",
    "\n",
    "D.plot_transects(location='mar', tr_id=10, dt=['dt_2','dt_3'], classified=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a788f",
   "metadata": {},
   "source": [
    "In the above example, we can see how a major sand nourishment interention built a considerable foredune during dt_2. However, as it is shown in dt_3, this intervation did't last long, as the sand got eroded and the profile started receding again.\n",
    "\n",
    "To have a full overview of a transect history all in one place, Sandpyper can plot it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25864145",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.plot_transect_mecs(location='mar',tr_id=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99dae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.plot_transect_mecs(location='mar', lod=D.lod_df, tr_id=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9cfd4e",
   "metadata": {},
   "source": [
    "Or, we can also have use Sandpyper to plot both alongshore heatmaps of altimetric change and alongshore estimated volume changes, in all locations and all time-periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d3a6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "D.plot_alongshore_change(mode='subset',       # to plot only a subset of locations and dts, rather than all\n",
    "                         dt_subset=['dt_2', 'dt_3'],\n",
    "                         location_subset=[\"mar\"],\n",
    "                         lod=D.lod_df, # LoD will be applied when provided in form of lod dataframe or single (global) value\n",
    "                         ax2_y_lims=(-10, 20), # lineplot y axis limits\n",
    "                        fig_size=(10,6),\n",
    "                        font_scale=.9,\n",
    "                        plots_spacing=0, # spacing between the two plots. Try .1, it is also nice\n",
    "                        bottom=True,\n",
    "                        y_heat_bottom_limit=40, # how far seawar the heatmap will extent.\n",
    "                        transect_spacing=D.ProfileSet.transects_spacing,\n",
    "                        heat_yticklabels_freq=10,\n",
    "                        heat_xticklabels_freq=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2153f7bc",
   "metadata": {},
   "source": [
    "#### Location-level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d87f2d",
   "metadata": {},
   "source": [
    "So far we saw data and plots at the transect level. Let's see a few plots at the location level, starting with the *ProfileDynamics.plot_single_loc* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff34611",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.plot_single_loc([\"mar\"],None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b5d10",
   "metadata": {},
   "source": [
    "In the above plot we can appreciate the volume change timeseries of Marengo, with the solid line representing the period-specific change while the dashed line the cumulative change from the start of the monitoring.\n",
    "\n",
    "Alternatively, we can plot both locations MECs timeseries in order to compare their elevational changes through time, using the method *ProfileDynamics.plot_mec_evolution*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_mode can be 'auto', which means the x axis is location-specific, or 'equal', better for comparison.\n",
    "\n",
    "D.plot_mec_evolution(location_field=\"location\",x_limits=(-.2, .8), scale_mode='optimised',\n",
    "                     loc_order=[\"leo\",\"mar\"],\n",
    "                    x_binning=10,\n",
    "                    figure_size=(6, 7),\n",
    "                    font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025eaf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_mode can be 'auto', which means the x axis is location-specific, or 'equal', better for comparison.\n",
    "\n",
    "D.plot_mec_evolution(location_field=\"location\",x_limits=(-.2, .8), scale_mode='equal',\n",
    "                     loc_order=[\"leo\",\"mar\"],\n",
    "                    x_binning=10,\n",
    "                    figure_size=(6, 7),\n",
    "                    font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9593f0",
   "metadata": {},
   "source": [
    "Alternatively, if we want to change the x-axis of only specific locations, we can set the scale_mode='equal', provide x_lims, __and provide a dictionary where keys are the location codes and values the location specific limits.__ In this way we can keep all the analysis visually comparable, with some exceptions (different scale) provided with the dictionary in the __x_diff__ parameter.\n",
    "\n",
    "```python\n",
    "    x_diff = {'leo': [-0.12, 0.1]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27eb151",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_diff={'leo':[-0.12, 0.1]}\n",
    "\n",
    "D.plot_mec_evolution(location_field=\"location\",x_limits=(-.2, .8), scale_mode='equal',\n",
    "                     x_diff=x_diff,\n",
    "                     dates_step=15,\n",
    "                     loc_order=[\"leo\",\"mar\"],\n",
    "                    x_binning=10,\n",
    "                     sort_locations=False,\n",
    "                    figure_size=(6, 7),\n",
    "                    font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b8c24b",
   "metadata": {},
   "source": [
    "## Hotspot analysis: Local Indicator of Spatial Association (LISA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c1934",
   "metadata": {},
   "source": [
    "In order to discard spatial outliers and get a better representation of the beachface dynamics by at the same time reducing the number of points analysed (in real-world case this can be an important aspect), we use the Local Moran's I index, which is one of the most used spatial autocorrelation measure, avalibale thanks to the great work in the Python package __[Pysal](https://pysal.org/)__.\n",
    "\n",
    "The method __*ProfileDynamics.LISA_site_level*__ performs a Local Moran's I with False Discovery Rate (fdr) correction analysis for all the elevation change points in each survey in the __dh_df__ dataframe.\n",
    "\n",
    "This method can use __KNN-based, inverse distance weighted or binary distance-based__ spatial weight matrices. In this example, we model spatial relationships with a distance-based row standardised binary weight matrix with neighborhood radius of __35 m__, in order to include two adjacent transect without getting too far from the focal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.LISA_site_level(mode=\"distance\", distance_value=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9408b6e6",
   "metadata": {},
   "source": [
    "This method adds some hotspot analysis relevant columns to the dh_df table, most importantly:\n",
    "\n",
    "- false discovery rate threshold (column: lisa_fdr)\n",
    "- local Moran's I (column: lisa_I)\n",
    "- simulated pseudo p and z values (columns: lisa_p_sim, lisa_z_value)\n",
    "- Moran's scatter plot quadrant (column: lisa_q)\n",
    "\n",
    "__MORAN SCATTERPLOT__ (High-High cluster, High-Low spatial outlier, Low-Low cluster and Low-High spatial outlier)\n",
    "\n",
    "Although technically the High-High and Low-Low Moran's scatter plot quadrants represent statistically significant spatial clusters, we commonly refer to these points as hotspots and coldspots.\n",
    "\n",
    ">__IMPORTANT NOTE__: Hot versus cold spots do not necessary reflect positive and negative values, rather, they represent statistically significan changes in the magnitude of the process analysed (elevationc change). Thus, in a scenario of extremely high deposition, hotspots represents high values of deposition surrounded by other high values of deposition, while coldspots represent low values of deposition surrounded by other low values of deposition. The choice of Local Moran's I as LISA has been motivated by the fact that we want to represent dynamics as best as we can by removing spatial outliers at the location level and we are not necessarily interested (at this stage) at the actual human interpretation (deposition vs erosion).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e2807",
   "metadata": {},
   "source": [
    "Now that we have statistically significant hotspots of change, we can use those points to capture the most interesting areas of beachface change which we use to model behaviour.\n",
    "\n",
    ">__ANOTHER NOTE__:\n",
    "Our beachfaces are narrow, and we use only reliable valid points using multiple levels of filtering (LoD, beachface area, sand-only), which significantly reduce the total number of usable points in each timestep.\n",
    "Because in our next step we will compute the Beachface Cluster Dynamics indices both at the location and transect scales, we will use the hotspot filtering only at the location level and not at the transect level.\n",
    "This is necessary to assure that we have enough points in each transect to model their behaviours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.hotspots.query(\"location=='mar' and dt=='dt_2' and lisa_q in [1,3]\").plot(column='lisa_q', categorical=True,\n",
    "                                                                            cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d568ec",
   "metadata": {},
   "source": [
    "## Multiscale Beachface Cluster Dynamics (BCDs) indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69f91b",
   "metadata": {},
   "source": [
    "In this section, the Beachface Cluster Dynamics (BCDs), which compirse both the empirical and residual BCD indices, are computed with sandpyper.\n",
    "\n",
    "> From [Pucino et al.](https://www.nature.com/articles/s41598-021-83477-6#Abs1),: \" *The empirical and residual BCDs (e-BCD and r-BCD respectively) are purposefully designed metrics to leverage the very high spatiotemporal resolutions and three-dimensionality of our data for studying subaerial beach landform dynamics (morphodynamics). With elevation change (Δh) magnitude classes as transient states, we used finite discrete Markov chain models to compute first-order stochastic transition matrices and steady-state probability vectors, used to derive e- and r-BCD respectively.*\" For more information about the methods, please refer to [the docs]().\n",
    "\n",
    "As stated above, before computing any BCD, we need to discretise the magnitude of elevation change that occurred at every time step into bins.\n",
    "When we intiate the ProfileSet instance we already provide sandpyper with all it need to perform the discretisation on the data.\n",
    "\n",
    "Therefore it is as simple as calling the *ProfileDynamics.discretise* method, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.discretise(absolute=True, print_summary=True, lod=D.lod_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67d4e1",
   "metadata": {},
   "source": [
    "By setting __absolute = True__, the bins will be derived from absolute values of elevation changes first, then, classes will be assigned to both negative and positive values accordingly. This is needed to ensure that both erosional and depositional classes represent the same interval in elevation change magnitudes, except of opposite sign.\n",
    "\n",
    "After this step, a new dataframe is added named __df_labelled__, which stores the __markov_tag__ field, which is the result of the discretisation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd783b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.df_labelled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58ebaa",
   "metadata": {},
   "source": [
    "Now, we are ready to compute the actual indices, but first, a note on the weights, neede for the e-BCDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b528a65",
   "metadata": {},
   "source": [
    "### e-BCDs weights\n",
    "\n",
    "The __e-BCD__ uses weights to represent the importance of each transition.<br>\n",
    "\n",
    "A point that transitioned from small erosion to extreme erosion has more impact on the sediment budget than if it was transitioning to a medium erosion class.<br>\n",
    "The weights for each magnitude class can be specified manually using a dictionary. However, it is best to obtain an objective representation of the severity of change. As the magnitude classes are derived from the data (using Jenks-Caspall optimised natural breaks method in this example), a simple yet robust solution is to use the medians of each magnitude class as weight.\n",
    "<br>\n",
    "<br>\n",
    "The function __infer_weights__ takes care of this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0da020",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.infer_weights()\n",
    "\n",
    "D.weights_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661e063",
   "metadata": {},
   "source": [
    "Of course, if we want to use our own designed weigths, it is as simple as this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2240b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the weight dictionary\n",
    "\n",
    "my_weights_dict={'Undefined_deposition': 1,\n",
    " 'Undefined_erosion': 1,\n",
    " 'Small_deposition': 1,\n",
    " 'Small_erosion': 1,\n",
    " 'Medium_deposition': 1,\n",
    " 'Medium_erosion': 1,\n",
    " 'High_deposition': 3,\n",
    " 'High_erosion': 3,\n",
    " 'Extreme_deposition': 8,\n",
    " 'Extreme_erosion': 8}\n",
    "\n",
    "# add it to the ProfileDynamics object\n",
    "\n",
    "D.weights_dict = my_weights_dict\n",
    "\n",
    "D.weights_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081c0d0",
   "metadata": {},
   "source": [
    "However, it makes more sense to use Sandpyper in-built method, so, let's recalculate it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8461df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.infer_weights()\n",
    "\n",
    "D.weights_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49357006",
   "metadata": {},
   "source": [
    "Now, we can compute BCDs at the location level at once.\n",
    "\n",
    "The method *ProfileDynamics.BCDs_compute* computes all the stochastic first-order transition matrices of sand dynamics, based on the filters provided (sand-only, hotspots, beyond LoD) across beachfaces, at the site level, which are the basis for the e-BCDs.\n",
    "\n",
    "It returns 2 dataframes:\n",
    "* __e-BCDs__\n",
    "* __steady-state distribution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.BCD_compute_location(\"geometry\",\"all\",True, filterit='lod')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c512ad",
   "metadata": {},
   "source": [
    "After this step, two new dataframes are added as attributes to the ProfileDynamics object:\n",
    "1. location_ebcds : stores the location level e-BCDs of each submatrix, as well as its value, sign and trend.\n",
    "2. location_ss : stores the location level steady-state vector of each magnitude of change class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b6c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.location_ebcds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.location_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199396c4",
   "metadata": {},
   "source": [
    "\n",
    "The residual is simply the difference between erosional and depositional probabilities in the Steady-State distribution, __multiplicated by 100__ for readability purposes. Note that in this case, no weigths are applied as no transition is represented.\n",
    "\n",
    "\n",
    "Here below, the dataframe returns the residual column, which is what you might want to map in Qgis.\n",
    "\n",
    "The above table, the row __r_bcds__ represent the r-BCD index for the locations, which we also call __behavioural regime__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3993de7",
   "metadata": {},
   "source": [
    "### Plot: first-order transition stochastic matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1513c5",
   "metadata": {},
   "source": [
    "Let's use sandpyper to visualise location-specific first-order stochastic matrices, representing the probability ofeach transiction, which is the basis for the computation of the e-BCDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following dictionary is used to rename the magnitude of change classes into small codes\n",
    "#such as 'me' for Medium Erosion or 'sd' for Small Deposition. This is done purely for graphical purposes.\n",
    "\n",
    "relabel_dict={'Undefined_deposition': 'ue',\n",
    " 'Undefined_erosion': 'ue',\n",
    " 'Small_deposition': 'sd',\n",
    " 'Small_erosion': 'se',\n",
    " 'Medium_deposition': 'md',\n",
    " 'Medium_erosion': 'me',\n",
    " 'High_deposition': 'hd',\n",
    " 'High_erosion': 'he',\n",
    " 'Extreme_deposition': 'ed',\n",
    " 'Extreme_erosion': 'ee'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#let's plot those matrices\n",
    "\n",
    "D.plot_trans_matrices(relabel_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a350b8",
   "metadata": {},
   "source": [
    "Notes: <br>\n",
    ">These matrices discard all the __valid to non-valid transitions__. In other words, all transitions going from a valid point (non spatial outlier, classified as sand and beyond limit of detection) are labelled as \"nnn\" and discarded.\n",
    "Moreover, __the colour ramp__ higher limit (vmax parameter) is set to a maximum of 3 times the standard deviation of all dataset without the nnn state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fecde5",
   "metadata": {},
   "source": [
    "#### How to read transition matrices\n",
    "\n",
    "__The title__ informs about:\n",
    "* mar/leo = Location codes (leo = St. Leonards; mar = Marengo)\n",
    "* n = Total number of points in the timeseries (valid and non-valid)\n",
    "* t = Total number of timesteps\n",
    "* trans = Total number of valid transitions considered\n",
    "\n",
    "__Rows__ are the initial state, while the __columns__ are the final (one-step) state. For instance, in Marengo, a point which starting state is Undefined Erosion (ue) has 12% probability to become a Medium Erosion (me) point and a 15% probability to become a Medium Deposition in the next time period.  \n",
    "\n",
    "__Some observations__\n",
    ">We chose St. Leonards as an example to demonstrates a few limitations, which are observable in this matrix.\n",
    "\n",
    "__St. Leonards__ is located within Port Phillip bay. This narrrow beach is not embayed, but __fetch-limited__, meaning that it doesn't receive the highly energetic Souther Ocean swell, rather, it sees its morphodynamics impacted by considerably lower waves which are locally generated from within the bay. Moreover, seagrass meadows and reefs further reduce wave imapcts on the subaerial beachface.<br>\n",
    "Therefore, __changes are small__ and often below the limit of detections, thus, uncertain. If we consider that we filter points based on:\n",
    "\n",
    "1. only sand\n",
    "2. limit of detection\n",
    "3. spatial outliers\n",
    "\n",
    "from an already narrow beach (short transects) this location is severly decimated in terms of behavioural modelling potential. In fact, the submatrices have many zeroes, which indicate a zero probability of these transitions, because these have not been recorded during the monitoring period.\n",
    "Moreover, we note how most of the transitions tend to be __from lower magnitude classes to low magnitude classes.__\n",
    "Another story for Marengo. <br>\n",
    "\n",
    "__Marengo__ is an open-ocean beach with a small southern section pretected by a small headland (tombolo-like). The section used for this example is also affected by sand nourishment, which injects considerable amount of sediment during the monitoring period.<br>\n",
    "Given in Marengo we have roughly 4 times the number of valid observations (trans=5'278) as compared to St. Leonards (trans=1'072), the submatrices better capture the stochastic behaviour of this beachface.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8dfe7f",
   "metadata": {},
   "source": [
    "### Plot: e-BCDs (location)\n",
    "\n",
    "Let's plot the e-BCDs, which summarises the information contained in the submatrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d91cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.plot_location_ebcds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44a759",
   "metadata": {},
   "source": [
    "### Plot: r-BCD (location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae3b38",
   "metadata": {},
   "source": [
    "We already computed the __Steady State__ probability vectors for each location using the __BCDs_compute__ function, storing it in the __ss__ variable.<br>\n",
    "Here below we reorder the data and plot as an heatmap, where each columns is a location and each row a magnitude of change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d14115",
   "metadata": {},
   "outputs": [],
   "source": [
    "order=[i for i in D.tags_order if i !='nnn']\n",
    "ss=D.location_ss.loc[order] \n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "sb.set_context(\"paper\", font_scale=2)\n",
    "\n",
    "\n",
    "f,ax=plt.subplots(figsize=(12,10))\n",
    "\n",
    "sb.heatmap(ss, cmap=\"Blues\",annot=True,\n",
    "           annot_kws={'size':14},linewidths=1,linecolor=\"white\", cbar_kws={'label': 'Lim. Probabilities'});\n",
    "\n",
    "#f.savefig(r'E:\\\\path\\\\to\\\\save\\\\picture.png', dpi=600); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384455c",
   "metadata": {},
   "source": [
    "Location level BCDs are good to compare multiple locations across wide areas. But what about getting a more detailed spatial-explicit view of the behaviour of a beachface system, at the __transect-level__?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a3e1f",
   "metadata": {},
   "source": [
    "### Transect-level BCDs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad3d1c",
   "metadata": {},
   "source": [
    "If we are looking at spatial distribution of r-BCDs, it is important that transects are comparable to each other within one location. \n",
    "\n",
    "So far, LoDs and sand-only filters have assured that only high quality data was used in the r-BCDs computation. At the transect level we add two additional conditions, to ensure that are only retained:\n",
    "\n",
    "1. points that remain valid for at least a certain amount of time periods\n",
    "2. transects that have a number of valid points greater than a determined minimum threshold\n",
    "\n",
    "Therefore, the choice of the two parameters (__thresh and min_pts__) of the *ProfileDynamics.BCD_compute_transects* method is very important:\n",
    "\n",
    "1. __min_points__: the minimum required valid points per transect to consider a transect reliable.\n",
    "2. __thresh__: the minimum number of timesteps required to retain a transect.\n",
    "\n",
    "This is done to ensure comparability across time and transects in a determined location.\n",
    "Therefore, __thresh__ is an important parameter that should ideally be as high as the available time periods, in order to ensure that the maximum behavioural variability is captured.<br>\n",
    "Yet, setting this parameter very high can reduce considerably the number of valid points retained in a single transect, which in turn can fall below __min_pt__, leading to the loss of transects from the final behavioural map.<br>\n",
    "\n",
    "\n",
    "Let's see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_specs={'mar':{'thresh':6,\n",
    "       'min_points':6}}\n",
    "\n",
    "D.BCD_compute_transects(loc_specs=loc_specs,reliable_action='keep', dirNameTrans=D.ProfileSet.dirNameTrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.transects_rbcd.query(\"location=='leo'\").plot(column=\"residual\", cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c89f4",
   "metadata": {},
   "source": [
    "Done. We now have transect level r-BCDS computed with the totality of elevation changes transitions occurred during the monitoring period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1cc2ad",
   "metadata": {},
   "source": [
    "#### Sensitivity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046224a6",
   "metadata": {},
   "source": [
    "However, as previously discussed, the choice of the __thresh and min_pts__ parameter is very important. It is also informative to monitor and try to minimise the number of transects that __passed from a depositional to erosional behavioral regime (or vice versa) in the last time period (i. e. changed sign from t-1 to t),__ for any chosen __thresh__. Those transects could signal a behaviour that only emerged by choosing a determined value for __thresh__, signaling a potentially __lower confidence in their r-BCD values__.<br>Also  choosing the right min_pts and thresh parameters is important.\n",
    "\n",
    "This is why sandpyper includes 2 functions to perform a __sensitivity analysis__:\n",
    "\n",
    "1. __sensitivity_tr_rbcd()__ : to perform a sensitivity analysis \n",
    "2. __plot_sensitivity_rbcds_transects()__ : to visualise the sensitivity analysis results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e40382",
   "metadata": {},
   "source": [
    "We start by defining all the combinations between the parameters __thresh (t)__ and __min_pts (pt)__, given ranges that we define.<br>\n",
    "The thresh parameter can be up to the total number of timesteps available, while the min_pts, we decide to test the values from 0 to 210, with a step of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_tr_big = sensitivity_tr_rbcd(D.df_labelled,\n",
    "                                test_thresholds='max',\n",
    "                                test_min_pts=[0,20,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(figsize=(10,10))\n",
    "\n",
    "palette=sb.color_palette( n_colors=ss_tr_big.tr_id.unique().shape[0])\n",
    "sb.lineplot(data=ss_tr_big, x='thresh',y='residual', hue='tr_id', style='location',\n",
    "            palette=palette, legend=False, **dict(alpha=0.5),\n",
    "            ax=ax\n",
    ")\n",
    "ax.set_ylabel(\"r_bcd\")\n",
    "ax.axhline(y=0, lw=2, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9e12d",
   "metadata": {},
   "source": [
    "As we can see, there are only 3 transects that flipped their r-BCD sign (depo to ero or viceversa) as a consequence of an increase in the minimum number of timesteps.\n",
    "\n",
    "In order to better decide a sub-optimal combination of thresh and min_pts that retains the majority of the transects (while keeping an eye on transects that flipped r-BCD), here below we do some plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6cc1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity_rbcds_transects(ss_tr_big, location='mar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ced0b5",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72446987",
   "metadata": {},
   "source": [
    "This notebook concludes the tutorials for the profile-based monitoring approach offered by sandpyper. In the next notebook we introduce a paradigm shift, which is raster-based analysis. However, this part of sandpyper is currently under development, thus, only a small __teaser notebook__ will illustrate a few useful functions already implemented in sandpyper.\n",
    "\n",
    "See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
