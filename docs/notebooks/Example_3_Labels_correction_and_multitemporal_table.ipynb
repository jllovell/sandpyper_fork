{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225cff23",
   "metadata": {},
   "source": [
    "# Example 3 - Labels correction and multitemporal table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-indonesian",
   "metadata": {},
   "source": [
    "<img src=\"images/banner3.png\" width=\"100%\" />\n",
    "\n",
    "<font face=\"Calibri\">\n",
    "<br>\n",
    "<font size=\"5\"> <b>Sand classification, beachface clipping and multitemporal analysis</b></font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Nicolas Pucino; PhD Student @ Deakin University, Australia </b> <br>\n",
    "<img style=\"padding:7px;\" src=\"images/sandpiper_sand_retouched.png\" width=\"170\" align=\"right\" /></font>\n",
    "\n",
    "<font size=\"3\">This notebook illustrates how to use assign the final Sand or no-sand labels to the points, clip only beachface areas and create an organised dataframe storing elevation changes from each period available in all locations. <br>\n",
    "\n",
    "<b>This notebook covers the following concepts:</b>\n",
    "\n",
    "- Sand vs No-Sand classification.\n",
    "- Beachface clipping.\n",
    "- Multitemporal extraction\n",
    "</font>\n",
    "\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satisfied-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "revolutionary-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_dataset=pd.read_csv(r\"C:\\my_packages\\doc_data\\labels\\data_classified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "european-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QGIS Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infectious-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionaries and lists\n",
    "\n",
    "labels_dict={\"water\":0,\n",
    "            \"sand\":1,\n",
    "            \"vegetation\":2,\n",
    "            \"no_sand\":3}\n",
    "\n",
    "labels_sand=[1]\n",
    "labels_no_sand=[0,2,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "floppy-liberia",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "water_dict= {'mar_20180601': [],\n",
    " 'inv_20201020': [],\n",
    " 'inv_20200826': []}\n",
    "\n",
    "no_sand_dict={'inv_20201211': [7],\n",
    " 'inv_20201020': [],\n",
    " 'inv_20200826': []}\n",
    "\n",
    "veg_dict={'inv_20201211': [],\n",
    " 'inv_20201020': [],\n",
    " 'inv_20200826': []}\n",
    "\n",
    "sand_dict={'mar_20180601: [3,5,8,9],\n",
    " 'inv_20201020': [0,2,3,6,7,8],\n",
    " 'inv_20200826': [1,3,4,7,8]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7216b21a",
   "metadata": {},
   "source": [
    "## Reclassification of sand labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "acceptable-cooler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4225f520c354aa399200fabad5eee54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv.\n",
      "Working on: inv_20201211.\n",
      "inv_20201211, evaluating WATER.\n",
      "inv_20201211, evaluating SAND.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npucino\\AppData\\Local\\Continuum\\anaconda3\\envs\\transectenvi\\lib\\site-packages\\ipykernel\\__main__.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_20201211, evaluating VEG.\n",
      "inv_20201211, evaluating NO_SAND.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npucino\\AppData\\Local\\Continuum\\anaconda3\\envs\\transectenvi\\lib\\site-packages\\ipykernel\\__main__.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In inv_20201211, the remaining labels [1, 4, 2, 6] are classified as no_sand.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\npucino\\AppData\\Local\\Continuum\\anaconda3\\envs\\transectenvi\\lib\\site-packages\\ipykernel\\__main__.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: inv_20201020.\n",
      "inv_20201020, evaluating WATER.\n",
      "inv_20201020, evaluating SAND.\n",
      "inv_20201020, evaluating VEG.\n",
      "inv_20201020, evaluating NO_SAND.\n",
      "In inv_20201020, the remaining labels [9, 5, 4, 1] are classified as no_sand.\n",
      "Working on: inv_20200826.\n",
      "inv_20200826, evaluating WATER.\n",
      "inv_20200826, evaluating SAND.\n",
      "inv_20200826, evaluating VEG.\n",
      "inv_20200826, evaluating NO_SAND.\n",
      "In inv_20200826, the remaining labels [6, 0, 9, 5, 2] are classified as no_sand.\n",
      "\n",
      "Checking for duplicated rows . . . \n",
      "Reclassification run successfully!\n",
      "Wall time: 5.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# classify sand VS no-sand\n",
    "\n",
    "df_labels=labelled_dataset\n",
    "\n",
    "corrected_labelled_df=pd.DataFrame()\n",
    "list_locs=labelled_dataset.location.unique()\n",
    "\n",
    "for location in tqdm(list_locs):\n",
    "    \n",
    "    print(f\"{location}.\")\n",
    "    \n",
    "    list_dates = df_labels.query(f\"location=='{location}'\").raw_date.unique()\n",
    "    \n",
    "    for survey_date in list_dates:\n",
    "                        \n",
    "        dataset_str=f\"{location}_{survey_date}\"\n",
    "        print(f\"Working on: {dataset_str}.\")\n",
    "        data_in=df_labels.query(f\"location=='{location}' & raw_date=='{survey_date}'\")\n",
    "        \n",
    "\n",
    "        list_labels=data_in.label_k.unique()   \n",
    "\n",
    "        # water\n",
    "        try:\n",
    "            print(f\"{dataset_str}, evaluating WATER.\")\n",
    "            label_df=data_in.query(f\"label_k == {water_dict[dataset_str]} \")\n",
    "            label_df[\"opt_label\"]=labels_dict[\"water\"]\n",
    "            corrected_labelled_df=pd.concat([label_df,corrected_labelled_df], ignore_index=True)\n",
    "            list_labels=[e for e in list_labels if e not in water_dict[dataset_str]]\n",
    "        \n",
    "        except:\n",
    "            print(f\"{dataset_str} does not have water classes\")\n",
    "\n",
    "        # sand\n",
    "        try:\n",
    "            print(f\"{dataset_str}, evaluating SAND.\")\n",
    "            label_df=data_in.query(f\"label_k == {sand_dict[dataset_str]} \")\n",
    "            label_df[\"opt_label\"]=labels_dict[\"sand\"]\n",
    "            corrected_labelled_df=pd.concat([label_df,corrected_labelled_df], ignore_index=True)\n",
    "            list_labels=[e for e in list_labels if e not in sand_dict[dataset_str]]\n",
    "\n",
    "        except:\n",
    "            print(f\"{dataset_str} does not have sand classes\")\n",
    "\n",
    "        # veg\n",
    "        try:\n",
    "            print(f\"{dataset_str}, evaluating VEG.\")\n",
    "            label_df=data_in.query(f\"label_k == {veg_dict[dataset_str]} \")\n",
    "            label_df[\"opt_label\"]=labels_dict[\"vegetation\"]\n",
    "            corrected_labelled_df=pd.concat([label_df,corrected_labelled_df], ignore_index=True)\n",
    "            list_labels=[e for e in list_labels if e not in veg_dict[dataset_str]]\n",
    "            \n",
    "        except:\n",
    "            print(f\"{dataset_str} does not have vegetation classes\")\n",
    "\n",
    "        # no_sand\n",
    "        try:\n",
    "            print(f\"{dataset_str}, evaluating NO_SAND.\")\n",
    "            label_df=data_in.query(f\"label_k == {no_sand_dict[dataset_str]} \")\n",
    "            label_df[\"opt_label\"]=labels_dict[\"no_sand\"]\n",
    "            corrected_labelled_df=pd.concat([label_df,corrected_labelled_df], ignore_index=True)\n",
    "            list_labels=[e for e in list_labels if e not in no_sand_dict[dataset_str]]\n",
    "        except:\n",
    "            print(f\"{dataset_str} does not have no_sand classes\")\n",
    "            \n",
    "            \n",
    "        \n",
    "        print(f\"In {dataset_str}, the remaining labels {list_labels} are classified as no_sand.\")\n",
    "            \n",
    "        label_df=data_in.query(f\"label_k == {list_labels} \")\n",
    "        label_df[\"opt_label\"]=labels_dict[\"no_sand\"]            \n",
    "        corrected_labelled_df=pd.concat([label_df,corrected_labelled_df], ignore_index=True)\n",
    "\n",
    "print(\"Checking for duplicated rows . . . \")\n",
    "\n",
    "if corrected_labelled_df.point_id.is_unique == False:\n",
    "    \n",
    "    print(\"There are some duplicated labels in the dictioanries. Run \"'duplicated_df.groupby(by=[\"location\",\"survey_date\",\"label_k\"]).count()'\" to find out. PLease correct them and re-run the cell. \")\n",
    "    # Select duplicate rows \n",
    "    duplicated_df = corrected_labelled_df[corrected_labelled_df.duplicated(['point_id'])]\n",
    "    duplicated_df.groupby(by=[\"location\",\"survey_date\",\"label_k\"]).count()\n",
    "else:\n",
    "    \n",
    "    print(\"Reclassification run successfully!\")\n",
    "\n",
    "\n",
    "corrected_labelled_df['sand_label'] = [ 1 if s in labels_no_sand else 0 for s in corrected_labelled_df.opt_label.values]\n",
    "corrected_labelled_df.sand_label.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "medieval-fetish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>z</th>\n",
       "      <th>tr_id</th>\n",
       "      <th>raw_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>location</th>\n",
       "      <th>survey_date</th>\n",
       "      <th>point_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>geometry</th>\n",
       "      <th>band1</th>\n",
       "      <th>band2</th>\n",
       "      <th>band3</th>\n",
       "      <th>slope</th>\n",
       "      <th>curve</th>\n",
       "      <th>label_k</th>\n",
       "      <th>opt_label</th>\n",
       "      <th>sand_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.416860</td>\n",
       "      <td>163</td>\n",
       "      <td>20200826</td>\n",
       "      <td>POINT (389485.0292927367 5722796.792901353)</td>\n",
       "      <td>inv</td>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>2601v22837030600in006</td>\n",
       "      <td>389485.0292927367</td>\n",
       "      <td>5722796.792901353</td>\n",
       "      <td>POINT (389485.029 5722796.793)</td>\n",
       "      <td>94.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>-2.766547</td>\n",
       "      <td>1.376038</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.419285</td>\n",
       "      <td>163</td>\n",
       "      <td>20200826</td>\n",
       "      <td>POINT (389485.0122613082 5722796.891440332)</td>\n",
       "      <td>inv</td>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>2601v22832000810in006</td>\n",
       "      <td>389485.0122613082</td>\n",
       "      <td>5722796.891440332</td>\n",
       "      <td>POINT (389485.012 5722796.891)</td>\n",
       "      <td>93.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>-0.002408</td>\n",
       "      <td>1.382086</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.421676</td>\n",
       "      <td>163</td>\n",
       "      <td>20200826</td>\n",
       "      <td>POINT (389484.9952298797 5722796.989979312)</td>\n",
       "      <td>inv</td>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>2601v22837070920in006</td>\n",
       "      <td>389484.9952298797</td>\n",
       "      <td>5722796.989979312</td>\n",
       "      <td>POINT (389484.995 5722796.990)</td>\n",
       "      <td>89.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-0.002375</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.424035</td>\n",
       "      <td>163</td>\n",
       "      <td>20200826</td>\n",
       "      <td>POINT (389484.9781984512 5722797.088518291)</td>\n",
       "      <td>inv</td>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>2601v22832050130in006</td>\n",
       "      <td>389484.97819845116</td>\n",
       "      <td>5722797.088518291</td>\n",
       "      <td>POINT (389484.978 5722797.089)</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-0.002343</td>\n",
       "      <td>-0.000652</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.426361</td>\n",
       "      <td>163</td>\n",
       "      <td>20200826</td>\n",
       "      <td>POINT (389484.9611670226 5722797.18705727)</td>\n",
       "      <td>inv</td>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>2601v22836020240in006</td>\n",
       "      <td>389484.96116702264</td>\n",
       "      <td>5722797.18705727</td>\n",
       "      <td>POINT (389484.961 5722797.187)</td>\n",
       "      <td>88.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-0.003679</td>\n",
       "      <td>-0.000640</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320347</td>\n",
       "      <td>92.5</td>\n",
       "      <td>3.377417</td>\n",
       "      <td>0</td>\n",
       "      <td>20201211</td>\n",
       "      <td>POINT (386871.7613396471 5721639.905428521)</td>\n",
       "      <td>inv</td>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>10204001i2272101nv95</td>\n",
       "      <td>386871.7613396471</td>\n",
       "      <td>5721639.905428521</td>\n",
       "      <td>POINT (386871.761 5721639.905)</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.012603</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320348</td>\n",
       "      <td>92.6</td>\n",
       "      <td>3.402572</td>\n",
       "      <td>0</td>\n",
       "      <td>20201211</td>\n",
       "      <td>POINT (386871.7460518774 5721640.004253033)</td>\n",
       "      <td>inv</td>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>10207001i2272401nv96</td>\n",
       "      <td>386871.7460518774</td>\n",
       "      <td>5721640.004253033</td>\n",
       "      <td>POINT (386871.746 5721640.004)</td>\n",
       "      <td>86.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>-0.006844</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320349</td>\n",
       "      <td>92.7</td>\n",
       "      <td>3.402572</td>\n",
       "      <td>0</td>\n",
       "      <td>20201211</td>\n",
       "      <td>POINT (386871.7307641076 5721640.103077544)</td>\n",
       "      <td>inv</td>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>10200001i2272601nv97</td>\n",
       "      <td>386871.7307641076</td>\n",
       "      <td>5721640.103077544</td>\n",
       "      <td>POINT (386871.731 5721640.103)</td>\n",
       "      <td>101.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.001086</td>\n",
       "      <td>-0.006841</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320350</td>\n",
       "      <td>92.8</td>\n",
       "      <td>3.400400</td>\n",
       "      <td>0</td>\n",
       "      <td>20201211</td>\n",
       "      <td>POINT (386871.7154763379 5721640.201902056)</td>\n",
       "      <td>inv</td>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>10203001i2272901nv98</td>\n",
       "      <td>386871.7154763379</td>\n",
       "      <td>5721640.201902056</td>\n",
       "      <td>POINT (386871.715 5721640.202)</td>\n",
       "      <td>103.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-0.001105</td>\n",
       "      <td>-0.006097</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320351</td>\n",
       "      <td>93.3</td>\n",
       "      <td>3.373779</td>\n",
       "      <td>0</td>\n",
       "      <td>20201211</td>\n",
       "      <td>POINT (386871.6390374891 5721640.696024614)</td>\n",
       "      <td>inv</td>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>10208001i2293101nv93</td>\n",
       "      <td>386871.6390374891</td>\n",
       "      <td>5721640.696024614</td>\n",
       "      <td>POINT (386871.639 5721640.696)</td>\n",
       "      <td>103.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-0.012963</td>\n",
       "      <td>-0.007356</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320352 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance         z tr_id  raw_date  \\\n",
       "0            0.0 -0.416860   163  20200826   \n",
       "1            0.1 -0.419285   163  20200826   \n",
       "2            0.2 -0.421676   163  20200826   \n",
       "3            0.3 -0.424035   163  20200826   \n",
       "4            0.4 -0.426361   163  20200826   \n",
       "...          ...       ...   ...       ...   \n",
       "320347      92.5  3.377417     0  20201211   \n",
       "320348      92.6  3.402572     0  20201211   \n",
       "320349      92.7  3.402572     0  20201211   \n",
       "320350      92.8  3.400400     0  20201211   \n",
       "320351      93.3  3.373779     0  20201211   \n",
       "\n",
       "                                        coordinates location survey_date  \\\n",
       "0       POINT (389485.0292927367 5722796.792901353)      inv  2020-08-26   \n",
       "1       POINT (389485.0122613082 5722796.891440332)      inv  2020-08-26   \n",
       "2       POINT (389484.9952298797 5722796.989979312)      inv  2020-08-26   \n",
       "3       POINT (389484.9781984512 5722797.088518291)      inv  2020-08-26   \n",
       "4        POINT (389484.9611670226 5722797.18705727)      inv  2020-08-26   \n",
       "...                                             ...      ...         ...   \n",
       "320347  POINT (386871.7613396471 5721639.905428521)      inv  2020-12-11   \n",
       "320348  POINT (386871.7460518774 5721640.004253033)      inv  2020-12-11   \n",
       "320349  POINT (386871.7307641076 5721640.103077544)      inv  2020-12-11   \n",
       "320350  POINT (386871.7154763379 5721640.201902056)      inv  2020-12-11   \n",
       "320351  POINT (386871.6390374891 5721640.696024614)      inv  2020-12-11   \n",
       "\n",
       "                     point_id                   x                  y  \\\n",
       "0       2601v22837030600in006   389485.0292927367  5722796.792901353   \n",
       "1       2601v22832000810in006   389485.0122613082  5722796.891440332   \n",
       "2       2601v22837070920in006   389484.9952298797  5722796.989979312   \n",
       "3       2601v22832050130in006  389484.97819845116  5722797.088518291   \n",
       "4       2601v22836020240in006  389484.96116702264   5722797.18705727   \n",
       "...                       ...                 ...                ...   \n",
       "320347   10204001i2272101nv95   386871.7613396471  5721639.905428521   \n",
       "320348   10207001i2272401nv96   386871.7460518774  5721640.004253033   \n",
       "320349   10200001i2272601nv97   386871.7307641076  5721640.103077544   \n",
       "320350   10203001i2272901nv98   386871.7154763379  5721640.201902056   \n",
       "320351   10208001i2293101nv93   386871.6390374891  5721640.696024614   \n",
       "\n",
       "                              geometry  band1  band2 band3     slope  \\\n",
       "0       POINT (389485.029 5722796.793)   94.0   91.0  95.0 -2.766547   \n",
       "1       POINT (389485.012 5722796.891)   93.0   89.0  93.0 -0.002408   \n",
       "2       POINT (389484.995 5722796.990)   89.0   85.0  89.0 -0.002375   \n",
       "3       POINT (389484.978 5722797.089)   85.0   83.0  86.0 -0.002343   \n",
       "4       POINT (389484.961 5722797.187)   88.0   85.0  91.0 -0.003679   \n",
       "...                                ...    ...    ...   ...       ...   \n",
       "320347  POINT (386871.761 5721639.905)   97.0  116.0  70.0  0.012603   \n",
       "320348  POINT (386871.746 5721640.004)   86.0  112.0  61.0  0.012578   \n",
       "320349  POINT (386871.731 5721640.103)  101.0  128.0  76.0 -0.001086   \n",
       "320350  POINT (386871.715 5721640.202)  103.0  127.0  76.0 -0.001105   \n",
       "320351  POINT (386871.639 5721640.696)  103.0  112.0  87.0 -0.012963   \n",
       "\n",
       "           curve  label_k  opt_label  sand_label  \n",
       "0       1.376038        6          3           1  \n",
       "1       1.382086        6          3           1  \n",
       "2       0.000033        0          3           1  \n",
       "3      -0.000652        0          3           1  \n",
       "4      -0.000640        0          3           1  \n",
       "...          ...      ...        ...         ...  \n",
       "320347  0.006434        9          1           0  \n",
       "320348 -0.006844        9          1           0  \n",
       "320349 -0.006841        9          1           0  \n",
       "320350 -0.006097        9          1           0  \n",
       "320351 -0.007356        9          1           0  \n",
       "\n",
       "[320352 rows x 19 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a geodataframe with point objects\n",
    "\n",
    "corrected_labelled_df = corrected_labelled_df.loc[:,~corrected_labelled_df.columns.duplicated()] # eliminate duplicated columns\n",
    "corrected_labelled_df['geometry']=corrected_labelled_df.coordinates.apply(coords_to_points) # create Point objects\n",
    "corrected_labelled_gdf=gpd.GeoDataFrame(corrected_labelled_df, geometry='geometry', crs=crs_dict_string['inv']) # create GeoDataFrame\n",
    "corrected_labelled_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04568038",
   "metadata": {},
   "source": [
    "## Polygon correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "appropriate-threshold",
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "E:\\chapter_4\\chloe_inv\\add3\\labels\\chloe_add3_Not_Sand.gpkg: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: E:\\chapter_4\\chloe_inv\\add3\\labels\\chloe_add3_Not_Sand.gpkg: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\conda3\\envs\\sandpiper_env\\lib\\site-packages\\geopandas\\io\\file.py\u001b[0m in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[1;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda3\\envs\\sandpiper_env\\lib\\site-packages\\fiona\\env.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda3\\envs\\sandpiper_env\\lib\\site-packages\\fiona\\__init__.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[1;32m--> 257\u001b[1;33m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda3\\envs\\sandpiper_env\\lib\\site-packages\\fiona\\collection.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDriverError\u001b[0m: E:\\chapter_4\\chloe_inv\\add3\\labels\\chloe_add3_Not_Sand.gpkg: No such file or directory"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# apply poly correction with geopandas\n",
    "\n",
    "#labelled_gdf=corrected_labelled_gdf\n",
    "corr_date_field='survey_date'\n",
    "labelled_df_date_field='raw_date'\n",
    "\n",
    "is_sand_path=r\"E:\\chapter_4\\chloe_inv\\add3\\labels\\chloe_add3_ToSand.gpkg\"\n",
    "is_not_sand_path=r\"E:\\chapter_4\\chloe_inv\\add3\\labels\\chloe_add3_Not_Sand.gpkg\"\n",
    "shore_path=r\"C:\\jupyter\\shore_areas\\inv_shore.gpkg\"\n",
    "\n",
    "#____________\n",
    "chloe_vali_noSand=gpd.read_file(is_not_sand_path)\n",
    "\n",
    "if '-' in chloe_vali_noSand.loc[:,corr_date_field].any():\n",
    "    chloe_vali_noSand.loc[:,corr_date_field]=chloe_vali_noSand.loc[:,corr_date_field].apply(lambda x: x.replace('-',''))\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if corr_date_field != 'date_raw':\n",
    "    chloe_vali_noSand.rename({corr_date_field:'date_raw'}, axis=1, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "chloe_vali_Sand=gpd.read_file(is_sand_path)\n",
    "\n",
    "if '-' in chloe_vali_Sand.loc[:,corr_date_field].any():\n",
    "    chloe_vali_Sand.loc[:,corr_date_field]=chloe_vali_Sand.loc[:,corr_date_field].apply(lambda x: x.replace('-',''))\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if corr_date_field != 'date_raw':\n",
    "    chloe_vali_Sand.rename({corr_date_field:'date_raw'}, axis=1, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "# check wether the correction geopackages are empty or not. If empty, skip correction.\n",
    "if chloe_vali_Sand.empty:\n",
    "    skip_isSand=True\n",
    "    print(\"Correction TO SAND skipped as the provided file is empty.\")\n",
    "else:\n",
    "    skip_isSand=False\n",
    "    \n",
    "if chloe_vali_noSand.empty:\n",
    "    skip_NoSand=True\n",
    "    print(\"Correction TO NO-SAND skipped as the provided file is empty.\")\n",
    "else:\n",
    "    skip_NoSand=False\n",
    "    \n",
    "\n",
    "# Poly correction starts here______________________________________\n",
    "\n",
    "to_update=pd.DataFrame()\n",
    "\n",
    "for date_in in labelled_gdf.loc[:,labelled_df_date_field].unique():\n",
    "\n",
    "    #subset points and polygones based on date\n",
    "    data_in=labelled_gdf.query(f\"{labelled_df_date_field} == '{date_in}'\")\n",
    "    vali_isSand=chloe_vali_Sand.query(f\"date_raw == '{date_in}'\")\n",
    "    vali_NoSand=chloe_vali_noSand.query(f\"date_raw == '{date_in}'\")\n",
    "    \n",
    "    # first set what IS SAND\n",
    "    if bool(skip_isSand)==True or vali_isSand.empty:\n",
    "        pass\n",
    "    else:\n",
    "        for i in range(vali_isSand.shape[0]): # loops through all the polygones\n",
    "\n",
    "            target_k=int(vali_isSand.iloc[i]['target_label_k'])\n",
    "\n",
    "            if target_k != 999:\n",
    "\n",
    "                data_in=labelled_gdf.query(f\"{labelled_df_date_field} == '{date_in}' & label_k=='{target_k}'\")\n",
    "                selection=data_in[data_in.geometry.intersects(vali_isSand.geometry.iloc[i])]\n",
    "                selection[\"corr_label\"]=0\n",
    "\n",
    "            elif target_k == 999:\n",
    "\n",
    "                data_in=labelled_gdf.query(f\"{labelled_df_date_field} == '{date_in}'\")\n",
    "                selection=data_in[data_in.geometry.intersects(vali_isSand.geometry.iloc[i])]\n",
    "                selection[\"corr_label\"]=0\n",
    "\n",
    "        to_update=pd.concat([selection,to_update], ignore_index=True)\n",
    "        \n",
    "            \n",
    "    # Now set what IS NOT SAND\n",
    "    if skip_NoSand or vali_NoSand.empty:\n",
    "        pass\n",
    "    else:\n",
    "        for i in range(vali_NoSand.shape[0]): # loops through all the polygones\n",
    "\n",
    "            target_k=int(vali_NoSand.iloc[i]['target_label_k'])\n",
    "\n",
    "            if target_k != 999:\n",
    "\n",
    "                data_in=labelled_gdf.query(f\"{labelled_df_date_field} == '{date_in}' & label_k=='{target_k}'\")\n",
    "                selection=data_in[data_in.geometry.intersects(vali_NoSand.geometry.iloc[i])]\n",
    "                selection[\"corr_label\"]=1\n",
    "\n",
    "            elif target_k == 999:\n",
    "\n",
    "                data_in=labelled_gdf.query(f\"{labelled_df_date_field} == '{date_in}'\")\n",
    "                selection=data_in[data_in.geometry.intersects(vali_NoSand.geometry.iloc[i])]\n",
    "                selection[\"corr_label\"]=1\n",
    "\n",
    "        to_update=pd.concat([selection,to_update], ignore_index=True)\n",
    "    \n",
    "to_update.drop_duplicates(subset=\"point_id\", inplace=True)\n",
    "\n",
    "\n",
    "#__CREATE NEW UPDATED DATAFRAME____________________________\n",
    "labelled_df_updated=pd.merge(left=labelled_gdf, right=to_update.loc[:,['point_id','corr_label']], # Left Join \n",
    "                             how='left', validate='one_to_one') \n",
    "labelled_df_updated.corr_label.fillna(labelled_df_updated.sand_label, inplace=True) # Fill NaN with previous sand labels\n",
    "labelled_df_updated[\"corr_label\"]=labelled_df_updated.corr_label.astype(int) # Transform corr_labels in Int\n",
    "\n",
    "#__CLIP BY SHORE____________________________\n",
    "shore=gpd.read_file(shore_path)\n",
    "in_shore=labelled_df_updated[labelled_df_updated.geometry.intersects(shore.geometry.iloc[0])]\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "in_shore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "novel-appeal",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_shore.to_csv(r\"E:\\chapter_4\\chloe_inv\\add3\\labels\\add3_inv_labelled_inshore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd28f04",
   "metadata": {},
   "source": [
    "## Create multitemporal datased (dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150b6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multitemporal Extraction to loop trough locations\n",
    "\n",
    "full_dataset=pd.read_csv(r\"C:\\my_packages\\doc_data\\profiles\\classified_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "constant-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multitemporal Extraction to loop trough locations\n",
    "\n",
    "def compute_multitemporal (df,\n",
    "                           date_field='survey_date',\n",
    "                          sand_label_field='label_sand',\n",
    "                          common_field=\"geometry\"):\n",
    "\n",
    "\n",
    "\n",
    "    fusion_long=pd.DataFrame()\n",
    "\n",
    "    for location in full_dataset.location.unique():\n",
    "        print(f\"working on {location}\")\n",
    "        loc_data=full_dataset.query(f\"location=='{location}'\")\n",
    "        list_dates=loc_data.loc[:,date_field].unique()\n",
    "        list_dates.sort()\n",
    "\n",
    "\n",
    "        for i in tqdm(range(list_dates.shape[0])):\n",
    "\n",
    "            if i < list_dates.shape[0]-1:\n",
    "                date_pre=list_dates[i]\n",
    "                date_post=list_dates[i+1]\n",
    "                print(f\"Calculating dt{i}, from {date_pre} to {date_post} in {location}.\")\n",
    "\n",
    "                df_pre=loc_data.query(f\"{date_field} =='{date_pre}' & {sand_label_field} == 0\").dropna(subset=['z'])\n",
    "                df_post=loc_data.query(f\"{date_field} =='{date_post}' & {sand_label_field} == 0\").dropna(subset=['z'])\n",
    "\n",
    "                merged=pd.merge(df_pre,df_post, how='inner', on=common_field,validate=\"one_to_one\",suffixes=('_pre','_post'))\n",
    "                merged[\"dh\"]=merged.z_post.astype(float) - merged.z_pre.astype(float)\n",
    "\n",
    "                dict_short={\"geometry\": merged.geometry,\n",
    "                            \"location\":location,\n",
    "                            \"tr_id\":merged.tr_id_pre,\n",
    "                            \"distance\":merged.distance_pre,\n",
    "                            \"dt\":  f\"dt_{i}\",\n",
    "                            \"date_pre\":date_pre,\n",
    "                            \"date_post\":date_post,\n",
    "                            \"z_pre\":merged.z_pre.astype(float),\n",
    "                            \"z_post\":merged.z_post.astype(float),\n",
    "                            \"dh\":merged.dh}\n",
    "\n",
    "                short_df=pd.DataFrame(dict_short)\n",
    "                fusion_long=pd.concat([short_df,fusion_long],ignore_index=True)\n",
    "\n",
    "    print(\"done\")\n",
    "    return fusion_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d31782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on mar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c828be3885b41f2b4f52034a6f9bab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating dt0, from 2018-06-01 to 2018-06-21 in mar.\n",
      "Calculating dt1, from 2018-06-21 to 2018-07-27 in mar.\n",
      "Calculating dt2, from 2018-07-27 to 2018-09-25 in mar.\n",
      "Calculating dt3, from 2018-09-25 to 2018-11-13 in mar.\n",
      "Calculating dt4, from 2018-11-13 to 2018-12-11 in mar.\n",
      "Calculating dt5, from 2018-12-11 to 2019-02-05 in mar.\n",
      "Calculating dt6, from 2019-02-05 to 2019-03-13 in mar.\n",
      "Calculating dt7, from 2019-03-13 to 2019-05-16 in mar.\n",
      "working on leo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01447a6eb7645cdb04e45f8a59e76be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating dt0, from 2018-06-06 to 2018-07-13 in leo.\n",
      "Calculating dt1, from 2018-07-13 to 2018-07-25 in leo.\n",
      "Calculating dt2, from 2018-07-25 to 2018-09-20 in leo.\n",
      "Calculating dt3, from 2018-09-20 to 2019-02-11 in leo.\n",
      "Calculating dt4, from 2019-02-11 to 2019-03-28 in leo.\n",
      "Calculating dt5, from 2019-03-28 to 2019-07-31 in leo.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "dh_df=compute_multitemporal(full_dataset,\n",
    "                      date_field='survey_date',\n",
    "                      sand_label_field='label_sand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "female-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_df.to_csv(r\"C:\\my_packages\\doc_data\\profiles\\dh_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3baa3cb",
   "metadata": {},
   "source": [
    "## Create Dataframe of details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c32b5046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>date_pre</th>\n",
       "      <th>date_post</th>\n",
       "      <th>location</th>\n",
       "      <th>n_days</th>\n",
       "      <th>loc_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt_0</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>2018-06-21</td>\n",
       "      <td>mar</td>\n",
       "      <td>20</td>\n",
       "      <td>Marengo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dt_1</td>\n",
       "      <td>2018-06-21</td>\n",
       "      <td>2018-07-27</td>\n",
       "      <td>mar</td>\n",
       "      <td>36</td>\n",
       "      <td>Marengo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dt_2</td>\n",
       "      <td>2018-07-27</td>\n",
       "      <td>2018-09-25</td>\n",
       "      <td>mar</td>\n",
       "      <td>60</td>\n",
       "      <td>Marengo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dt_3</td>\n",
       "      <td>2018-09-25</td>\n",
       "      <td>2018-11-13</td>\n",
       "      <td>mar</td>\n",
       "      <td>49</td>\n",
       "      <td>Marengo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dt_4</td>\n",
       "      <td>2018-11-13</td>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>mar</td>\n",
       "      <td>28</td>\n",
       "      <td>Marengo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dt_5</td>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>2019-02-05</td>\n",
       "      <td>mar</td>\n",
       "      <td>56</td>\n",
       "      <td>Marengo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dt_6</td>\n",
       "      <td>2019-02-05</td>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>mar</td>\n",
       "      <td>36</td>\n",
       "      <td>Marengo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dt_7</td>\n",
       "      <td>2019-03-13</td>\n",
       "      <td>2019-05-16</td>\n",
       "      <td>mar</td>\n",
       "      <td>64</td>\n",
       "      <td>Marengo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dt_0</td>\n",
       "      <td>2018-06-06</td>\n",
       "      <td>2018-07-13</td>\n",
       "      <td>leo</td>\n",
       "      <td>37</td>\n",
       "      <td>St. Leonards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dt_1</td>\n",
       "      <td>2018-07-13</td>\n",
       "      <td>2018-07-25</td>\n",
       "      <td>leo</td>\n",
       "      <td>12</td>\n",
       "      <td>St. Leonards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dt_2</td>\n",
       "      <td>2018-07-25</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>leo</td>\n",
       "      <td>57</td>\n",
       "      <td>St. Leonards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dt_3</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2019-02-11</td>\n",
       "      <td>leo</td>\n",
       "      <td>144</td>\n",
       "      <td>St. Leonards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dt_4</td>\n",
       "      <td>2019-02-11</td>\n",
       "      <td>2019-03-28</td>\n",
       "      <td>leo</td>\n",
       "      <td>45</td>\n",
       "      <td>St. Leonards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dt_5</td>\n",
       "      <td>2019-03-28</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>leo</td>\n",
       "      <td>125</td>\n",
       "      <td>St. Leonards</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dt    date_pre   date_post location  n_days      loc_full\n",
       "0   dt_0  2018-06-01  2018-06-21      mar      20       Marengo\n",
       "1   dt_1  2018-06-21  2018-07-27      mar      36       Marengo\n",
       "2   dt_2  2018-07-27  2018-09-25      mar      60       Marengo\n",
       "3   dt_3  2018-09-25  2018-11-13      mar      49       Marengo\n",
       "4   dt_4  2018-11-13  2018-12-11      mar      28       Marengo\n",
       "5   dt_5  2018-12-11  2019-02-05      mar      56       Marengo\n",
       "6   dt_6  2019-02-05  2019-03-13      mar      36       Marengo\n",
       "7   dt_7  2019-03-13  2019-05-16      mar      64       Marengo\n",
       "8   dt_0  2018-06-06  2018-07-13      leo      37  St. Leonards\n",
       "9   dt_1  2018-07-13  2018-07-25      leo      12  St. Leonards\n",
       "10  dt_2  2018-07-25  2018-09-20      leo      57  St. Leonards\n",
       "11  dt_3  2018-09-20  2019-02-11      leo     144  St. Leonards\n",
       "12  dt_4  2019-02-11  2019-03-28      leo      45  St. Leonards\n",
       "13  dt_5  2019-03-28  2019-07-31      leo     125  St. Leonards"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataframe of details\n",
    "\n",
    "# add full names to codes\n",
    "loc_full={'mar': 'Marengo',\n",
    "         'leo': 'St. Leonards'}\n",
    "\n",
    "locs_dt_str=pd.DataFrame()\n",
    "for location in dh_df.location.unique():\n",
    "   \n",
    "    df_time_tmp=dh_df.query(f\"location=='{location}'\").groupby(['dt'])[['date_pre','date_post']].first().reset_index()\n",
    "    df_time_tmp[\"orderid\"]=[int(i.split(\"_\")[1]) for i in df_time_tmp.dt]\n",
    "    df_time_tmp.sort_values([\"orderid\"], inplace=True)\n",
    "    df_time_tmp[\"location\"]=location   \n",
    "    locs_dt_str=pd.concat([df_time_tmp,locs_dt_str], ignore_index=True)\n",
    "\n",
    "# add days between dates\n",
    "deltas=[(datetime.strptime(d_to, '%Y-%m-%d') - datetime.strptime(d_from, '%Y-%m-%d')).days\n",
    "        for d_to,d_from in zip(locs_dt_str.date_post,locs_dt_str.date_pre)]\n",
    "locs_dt_str['n_days']=deltas\n",
    "\n",
    "# add full names to codes\n",
    "locs_dt_str['loc_full']=locs_dt_str.location.map(loc_full)\n",
    "\n",
    "# some cleaning and renaming\n",
    "locs_dt_str.drop('orderid',1,inplace=True)\n",
    "locs_dt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcd1f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_dt_str.to_csv(r\"C:\\my_packages\\doc_data\\profiles\\dt_info.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
